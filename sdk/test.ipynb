{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1002604/3036095861.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  save_dict = torch.load('/home/aw1223/agile/sdk/datasets/datasets_and_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_10_nodes\n",
      "Data(y=[10], edge_index=[2, 40], x=[10, 32], edge_attr=[40, 32])\n",
      "dataset_50_nodes\n",
      "Data(y=[50], edge_index=[2, 280], x=[50, 32], edge_attr=[280, 32])\n",
      "dataset_100_nodes\n",
      "Data(y=[100], edge_index=[2, 580], x=[100, 32], edge_attr=[580, 32])\n",
      "dataset_500_nodes\n",
      "Data(y=[500], edge_index=[2, 2982], x=[500, 32], edge_attr=[2982, 32])\n",
      "dataset_1000_nodes\n",
      "Data(y=[1000], edge_index=[2, 5984], x=[1000, 32], edge_attr=[5984, 32])\n",
      "Ensure inputs match the following list {'g2m_embedder_input_0': None, 'g2m_int_net_input_1': None, 'grid_mesh_embedder_input_0': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 13981.01it/s]\n",
      "6it [00:00, 34568.44it/s]\n",
      "2it [00:00, 17015.43it/s]\n",
      "6it [00:00, 37393.50it/s]\n",
      "2it [00:00, 15797.76it/s]\n",
      "6it [00:00, 39138.14it/s]\n",
      "2it [00:00, 17772.47it/s]\n",
      "6it [00:00, 36366.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.001192241372168064, 0.004840687133814812]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001192 |\n",
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0011525160905718804, 0.004600903240545307]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001153 |\n",
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n",
      "Ensure inputs match the following list {'g2m_embedder_input_0': None, 'g2m_int_net_input_1': None, 'grid_mesh_embedder_input_0': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 36884.58it/s]\n",
      "36it [00:00, 80919.05it/s]\n",
      "7it [00:00, 42924.16it/s]\n",
      "36it [00:00, 78848.53it/s]\n",
      "7it [00:00, 42123.57it/s]\n",
      "36it [00:00, 80962.44it/s]\n",
      "7it [00:00, 43176.66it/s]\n",
      "36it [00:00, 80487.71it/s]\n",
      "Process Process-7:1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:33:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:37:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:39:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:41:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:45:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:47:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename /home/aw1223/agile/timing_tmp.txt\n",
      "Inference job completed in [0.0010825218808054925, 0.004780238732114484]ms. Terminating power job...\n",
      "| Component   | Metric           |    Value |\n",
      "|:------------|:-----------------|---------:|\n",
      "| gpu         | Gpu Latency Mean | 0.001083 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw1223/agile/sdk/benchmarking_manager.py\", line 115, in gpu_run_inference\n",
      "    self.gpu_bman.model.to(torch.device(f\"cuda:{self.args.device}\"))\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "  File \"/home/aw1223/.conda/envs/ample/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interrupt detected. Terminating processes...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'lst' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m ample \u001b[38;5;241m=\u001b[39m Ample(gpu_sim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,plot \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mto_device(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mample\u001b[39m\u001b[38;5;124m'\u001b[39m,data\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m---> 39\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m outputs_dict[key] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs_model\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs_model,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_mesh_emb\u001b[39m\u001b[38;5;124m'\u001b[39m: grid_mesh_emb\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ample\n",
      "File \u001b[0;32m~/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ample/lib/python3.11/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/agile/sdk/ample.py:63\u001b[0m, in \u001b[0;36mAmple.overload_forward.<locals>.ample_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mample_forward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# if self.sim:\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#Change name\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m#     self.driver.load_layer_config()\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#     self.driver.load_regbanks()\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#     self.driver.execute()\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/agile/sdk/ample.py:91\u001b[0m, in \u001b[0;36mAmple.simulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\n\u001b[1;32m     77\u001b[0m     cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_sim,\n\u001b[1;32m     78\u001b[0m     gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_sim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     89\u001b[0m bman \u001b[38;5;241m=\u001b[39m BenchmarkingManager(inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39margs)\n\u001b[0;32m---> 91\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mbman\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m bman\u001b[38;5;241m.\u001b[39mprint_metrics(metrics)\n",
      "File \u001b[0;32m~/agile/sdk/benchmarking_manager.py:295\u001b[0m, in \u001b[0;36mBenchmarkingManager.benchmark\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_benchmark()\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgpu):\n\u001b[0;32m--> 295\u001b[0m    metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msim):\n\u001b[1;32m    297\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpga\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpga_benchmark()\n",
      "File \u001b[0;32m~/agile/sdk/benchmarking_manager.py:201\u001b[0m, in \u001b[0;36mBenchmarkingManager.gpu_benchmark\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_latency_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: lst[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_latency_std_dev\u001b[39m\u001b[38;5;124m\"\u001b[39m: lst[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_throughput_per_watt\u001b[39m\u001b[38;5;124m\"\u001b[39m: throughput\u001b[38;5;241m/\u001b[39mpower\n\u001b[1;32m    198\u001b[0m             }\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_latency_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mlst\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    202\u001b[0m     }\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'lst' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,\"/home/aw1223/agile\")\n",
    "\n",
    "import torch\n",
    "from sdk.models.graphcast import Graphcast\n",
    "from sdk.ample import Ample\n",
    "\n",
    "save_dict = torch.load('/home/aw1223/agile/sdk/datasets/datasets_and_model.pt')\n",
    "\n",
    "# Extract the datasets and model state dict\n",
    "data_dict = save_dict['datasets']\n",
    "model_state_dict = save_dict['model_state_dict']\n",
    "\n",
    "model = Graphcast()\n",
    "\n",
    "\n",
    "#Datasets\n",
    "for key, output in data_dict.items():\n",
    "    print(key)\n",
    "    print(output)\n",
    "    \n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "outputs_dict = {}\n",
    "for key, dataset in data_dict.items():\n",
    "    inputs = [\n",
    "        dataset.edge_attr,\n",
    "        dataset.edge_index,\n",
    "        dataset.x,\n",
    "        (dataset.edge_attr + 0.1),\n",
    "        dataset.edge_index\n",
    "    ]\n",
    "    \n",
    "    outputs_model, grid_mesh_emb = model(*inputs)\n",
    "    ample = Ample(gpu_sim=True,plot =False)\n",
    "    model.to_device('ample',data=inputs)\n",
    "    out = model(*inputs)\n",
    "    \n",
    "    outputs_dict[key] = {\n",
    "        'outputs_model': outputs_model,\n",
    "        'grid_mesh_emb': grid_mesh_emb\n",
    "    }\n",
    "    del ample\n",
    "torch.save(outputs_dict, 'model_outputs.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
