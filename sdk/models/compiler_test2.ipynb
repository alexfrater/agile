{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "outputs_sub_model lin 1\n",
      "[tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "          0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "         -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "         -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "        [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "          0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "         -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "          0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "        [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "         -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "         -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "         -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]],\n",
      "       grad_fn=<MmBackward0>)]\n",
      "outputs_sub_model lin 2\n",
      "[tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "         -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "         -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "         -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "         -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "          2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "         -4.3428e-01,  1.1919e-01],\n",
      "        [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "         -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "         -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "          2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "          3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "          5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "          4.9247e-01,  1.5074e-01],\n",
      "        [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "         -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "          3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "         -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "          8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "         -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "         -6.7968e-01, -1.9930e-01]], grad_fn=<MmBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# print('dataset.x' ,dataset.x)\n",
    "# print('dataset.edge_index' ,dataset.edge_index)\n",
    "# print('dataset.edge_attr' ,dataset.edge_attr)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0,'/home/aw1223/ip/agile')\n",
    "\n",
    "from sdk.ample import Ample\n",
    "\n",
    "from torch_geometric.datasets import FakeDataset #TODO remove\n",
    "from sdk.models.models import MLP_Model,Interaction_Net_Model\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "\n",
    "class ToyModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels=32, out_channels=32, layer_count=1, hidden_dimension=32, precision = torch.float32):\n",
    "        super().__init__()\n",
    "        self.precision = precision\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "\n",
    "\n",
    "        self.linear1 = MLP_Model(in_channels, out_channels) \n",
    "        self.linear1.name  = 'linear_embedder1'\n",
    "        self.layers.append(self.linear1) #Used to map weights in SDK\n",
    "\n",
    "\n",
    "        self.linear2 = MLP_Model(in_channels, out_channels) \n",
    "        self.linear2.name  = 'linear_embedder2'\n",
    "        self.layers.append(self.linear2) #Used to map weights in SDK\n",
    "\n",
    "        # self.int_net = Interaction_Net_Model(in_channels, out_channels, layer_count, hidden_dimension, precision)\n",
    "        # self.int_net.name  = 'int_net'\n",
    "        # self.layers.append(self.int_net) #Used to map weights in SDK\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.to(self.precision)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs_model = []\n",
    "        x = x.to(self.precision) \n",
    "        print(x) \n",
    "        outputs_sub_model1,x = self.linear1(x)\n",
    "        print('outputs_sub_model lin 1')\n",
    "        print(outputs_sub_model1)\n",
    "        #need to add a name to each model output as order of computation is not guaranteed\n",
    "        # outputs_model = outputs_model + outputs_sub_model #Add instead of append to have layer outputs in single list as tb iterates over each layer\n",
    "        # print(x)\n",
    "\n",
    "        outputs_sub_model2,x = self.linear2(x)\n",
    "        print('outputs_sub_model lin 2')\n",
    "        print(outputs_sub_model2)\n",
    "        outputs_model = outputs_sub_model1 + outputs_sub_model2\n",
    "        # _,x = self.int_net(x, edge_index, edge_attr)\n",
    "\n",
    "        return outputs_model,x\n",
    "\n",
    "model = ToyModel(32,32)\n",
    "\n",
    "dataset = Data()\n",
    "dataset.x = torch.tensor([[-2.0775e-01,  5.4138e-01, -9.3228e-01, -1.5647e-02, -9.2422e-01,\n",
    "         -1.4551e+00,  1.1447e+00,  4.8944e-01, -1.2190e+00, -2.1434e+00,\n",
    "          8.0335e-01, -1.3588e+00, -1.2911e+00,  1.1237e+00,  1.1250e-01,\n",
    "          3.9626e-01,  5.1411e-01,  9.9543e-01,  7.6991e-02,  1.1795e+00,\n",
    "         -1.2423e+00, -2.6467e-01, -4.9839e-01, -1.0298e+00, -2.2073e+00,\n",
    "          1.7572e+00, -5.1693e-01,  1.4884e+00,  1.1717e+00, -1.6047e+00,\n",
    "         -1.0268e+00,  1.6982e+00],\n",
    "        [-1.8529e+00,  1.6048e+00, -6.7576e-01,  4.8206e-01, -7.3550e-01,\n",
    "          2.7671e+00,  6.5742e-01,  1.1275e-01, -8.2426e-01,  9.4348e-01,\n",
    "         -5.4252e-01,  1.3881e+00, -3.2231e-01,  2.2466e+00,  2.9660e-01,\n",
    "         -3.3892e-01, -3.5646e-01,  1.7096e+00,  6.5559e-01,  9.3671e-01,\n",
    "         -2.0564e-01, -2.5100e-01, -6.0347e-02, -9.6708e-01,  2.2658e+00,\n",
    "          2.2228e-01, -2.8040e+00, -8.2614e-01,  5.2462e-01, -2.1695e+00,\n",
    "         -1.4330e-02,  8.4461e-01],\n",
    "        [ 6.7803e+00,  5.0186e+00,  5.3118e+00,  6.6033e+00,  5.3946e+00,\n",
    "          4.0241e+00,  5.3939e+00,  5.0030e+00,  4.8918e+00,  3.9511e+00,\n",
    "          6.0363e+00,  4.9824e+00,  5.2248e+00,  6.5227e+00,  5.0742e+00,\n",
    "          4.1273e+00,  4.4227e+00,  5.2284e+00,  5.5740e+00,  6.3545e+00,\n",
    "          4.3107e+00,  5.2994e+00,  5.3288e+00,  5.3148e+00,  5.8875e+00,\n",
    "          5.0108e+00,  5.8182e+00,  4.4662e+00,  5.2387e+00,  6.0119e+00,\n",
    "          4.5336e+00,  5.5987e+00]])\n",
    "dataset.edge_index = torch.tensor([[ 0,  0,  0,  1,  1,  1, 2,  2,  2],\n",
    "        [ 0,  1, 2,  0,  1,  2,  0,  1, 2]])\n",
    "dataset.edge_attr =  None\n",
    "\n",
    "\n",
    "inputs = [dataset.x]\n",
    "out = model(*inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "layers.0.layers.0.weight tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]])\n",
      "layers.1.layers.0.weight tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]])\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Moving model to Ample\n",
      "Compiling model\n",
      "model name ToyModel\n",
      "tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 1\n",
      "[tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "          0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "         -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "         -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "        [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "          0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "         -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "          0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "        [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "         -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "         -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "         -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]])]\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 2\n",
      "[tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "         -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "         -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "         -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "         -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "          2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "         -4.3428e-01,  1.1919e-01],\n",
      "        [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "         -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "         -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "          2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "          3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "          5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "          4.9247e-01,  1.5074e-01],\n",
      "        [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "         -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "          3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "         -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "          8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "         -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "         -6.7968e-01, -1.9930e-01]])]\n",
      "input_to_layer_map {('linear1_input_0', 0): 'linear1', ('linear1_output_1', 0): 'linear2'}\n",
      "External inputs {'linear1_input_0': tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])}\n",
      "key linear1_input_0\n",
      "key linear1_input_0\n",
      "value tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "Tensor\n",
      "not edge index\n",
      "not in edge attr\n",
      "key linear1_input_0\n",
      "MLP_Model\n",
      "name linear1\n",
      "check --------\n",
      "external_inputs_dict {'linear1_input_0': tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])}\n",
      "input_names ['linear1_input_0']\n",
      "sub module name external linear1\n",
      "sub_module_name linear1\n",
      "input_names ['linear1_input_0']\n",
      "edge_attr_external_inputs_dict {}\n",
      "edge_index_external_inputs_dict {}\n",
      "node_feature_external_inputs_dict {'linear1_input_0': tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])}\n",
      "name linear1_input_0\n",
      "node features\n",
      "node features\n",
      "AMPLE suub model MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "layers.0.weight torch.Size([32, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]],\n",
      "       requires_grad=True)\n",
      "base_path /home/aw1223/ip/agile/hw/sim/layer_config/\n",
      "attr check None\n",
      "linear\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0084,  0.0304,  0.1117,  0.1244, -0.0840,  0.0266, -0.0987, -0.0040,\n",
      "         0.1691, -0.1234,  0.1093, -0.1452,  0.1274, -0.0419,  0.0155,  0.0694,\n",
      "        -0.0970,  0.0632,  0.1515,  0.0494, -0.1331, -0.1611, -0.1382, -0.0618,\n",
      "        -0.0082, -0.1485,  0.1265, -0.0413, -0.0438, -0.0922, -0.1008, -0.1613],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0309,  0.0559,  0.0508, -0.0263, -0.1094,  0.1370, -0.0511,  0.1538,\n",
      "         0.0003, -0.1217, -0.1668, -0.0372,  0.0371, -0.0133,  0.0291, -0.1353,\n",
      "         0.0557, -0.1595, -0.1591, -0.0371, -0.0757, -0.0796,  0.1442,  0.0843,\n",
      "        -0.0772,  0.0411, -0.0670,  0.0389,  0.1338,  0.0325,  0.0554, -0.1621],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0070,  0.0611,  0.0005, -0.1034,  0.1092,  0.0656,  0.0763,  0.1384,\n",
      "        -0.0887,  0.0108, -0.0345,  0.0235,  0.1525, -0.0155, -0.0665,  0.1595,\n",
      "        -0.1421,  0.0938,  0.0798,  0.0295, -0.1569, -0.1585,  0.1018,  0.1698,\n",
      "         0.0413,  0.0970,  0.0746,  0.1446, -0.1520, -0.1199, -0.1162, -0.1191],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0495, -0.1672,  0.1390,  0.0092, -0.1435, -0.1103, -0.1439, -0.0151,\n",
      "        -0.1595, -0.1351,  0.1764,  0.0604, -0.1020,  0.1533, -0.0333, -0.0070,\n",
      "         0.1277, -0.0528,  0.1151, -0.1767,  0.0704,  0.0163,  0.1219, -0.0723,\n",
      "        -0.0323,  0.1120,  0.0819, -0.1602,  0.0666, -0.0951, -0.0630,  0.1169],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0979, -0.1730,  0.0348, -0.0021,  0.1492,  0.0344, -0.0394, -0.1386,\n",
      "         0.0085, -0.0002,  0.0014,  0.1060, -0.0982, -0.0776,  0.0007, -0.1496,\n",
      "         0.1759, -0.0647,  0.1359,  0.0506, -0.0048,  0.1102,  0.1313, -0.1005,\n",
      "         0.1453, -0.1078, -0.0708, -0.1472, -0.0616, -0.0960,  0.0328, -0.1349],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0699, -0.0604, -0.0047, -0.1728, -0.0196,  0.0321,  0.0419,  0.0369,\n",
      "        -0.1015, -0.1210, -0.0167, -0.0648,  0.1228, -0.1451, -0.0552,  0.1066,\n",
      "         0.0568,  0.1112,  0.0823, -0.1496, -0.0218,  0.0973,  0.1360, -0.0147,\n",
      "         0.1564,  0.0302, -0.0339, -0.0234,  0.0767, -0.0011,  0.1527, -0.0715],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1749,  0.0579,  0.1086,  0.1610,  0.1266, -0.1109,  0.0121,  0.0120,\n",
      "        -0.1643, -0.1056,  0.1338,  0.1176,  0.0715,  0.1696,  0.1634,  0.0466,\n",
      "        -0.0451,  0.0298, -0.0607, -0.0723,  0.0206, -0.0223, -0.1226, -0.0559,\n",
      "        -0.1303, -0.0235, -0.0181, -0.1055, -0.0444,  0.1456,  0.0542,  0.0759],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0930,  0.1143,  0.0964, -0.0194,  0.0080, -0.0066,  0.1130,  0.1353,\n",
      "         0.0095, -0.0499, -0.1064, -0.1208,  0.1502, -0.1031,  0.1556,  0.0540,\n",
      "         0.1398,  0.1559, -0.0177, -0.0955, -0.0033,  0.0754, -0.0174,  0.1127,\n",
      "         0.1039, -0.0981,  0.0562,  0.0821,  0.0887,  0.0438,  0.1226, -0.1348],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0256, -0.0357, -0.1758,  0.1271, -0.0455,  0.1003,  0.1326,  0.1721,\n",
      "        -0.0930, -0.1096, -0.0866, -0.0532, -0.1326,  0.0896,  0.0487,  0.1426,\n",
      "         0.0232,  0.0717,  0.0016, -0.1495,  0.1556, -0.0658, -0.1210,  0.0539,\n",
      "        -0.1012, -0.0299,  0.0165, -0.0123,  0.0952, -0.1590,  0.1747, -0.1152],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1138,  0.1338, -0.0166, -0.0552, -0.0705, -0.1242,  0.1593, -0.1319,\n",
      "        -0.1477,  0.1371, -0.1295, -0.0591, -0.0026,  0.0774,  0.1148,  0.1526,\n",
      "        -0.0226,  0.0213,  0.1401, -0.0573, -0.0562, -0.0368,  0.1570,  0.1530,\n",
      "        -0.0977,  0.0150, -0.1481,  0.0232, -0.0867, -0.0879,  0.0235,  0.0782],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-1.0982e-01,  2.1347e-02, -9.6312e-02,  3.3505e-02, -1.3529e-01,\n",
      "        -2.4101e-02,  4.4280e-02,  3.6110e-02, -1.3006e-01, -1.1601e-02,\n",
      "        -9.9816e-03,  5.9351e-02,  5.7052e-02, -1.3107e-01,  4.9950e-02,\n",
      "        -5.2747e-05,  8.6064e-02,  7.0330e-02,  3.8237e-02,  1.7226e-01,\n",
      "         8.3286e-02,  7.1408e-02, -1.1779e-01,  1.2279e-01, -1.0654e-01,\n",
      "        -9.2749e-02, -6.0054e-02, -1.5892e-01, -1.3139e-02, -1.2354e-01,\n",
      "         1.3296e-01, -6.6094e-02], grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1106,  0.0896,  0.1646,  0.0306, -0.0929,  0.1283, -0.0243, -0.0327,\n",
      "         0.0272, -0.1294,  0.0315,  0.0258,  0.0902,  0.0240, -0.1735, -0.0759,\n",
      "         0.0567, -0.0884, -0.0963,  0.0812, -0.0373, -0.1464, -0.0707,  0.0130,\n",
      "         0.0798, -0.1273,  0.0604, -0.0224,  0.0597, -0.1294,  0.0138,  0.1627],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1631, -0.1103,  0.0804, -0.0391, -0.0262,  0.1248, -0.1006, -0.1224,\n",
      "        -0.0841,  0.0799,  0.0435, -0.1463, -0.1677, -0.0440, -0.1360, -0.0267,\n",
      "         0.0164, -0.0609, -0.0691,  0.0884,  0.1489, -0.0180,  0.0800, -0.1567,\n",
      "         0.0953, -0.1236, -0.1125, -0.1010, -0.0329, -0.0166,  0.0309,  0.1035],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1539, -0.1504,  0.0923, -0.1377,  0.0433,  0.0270, -0.0319, -0.0497,\n",
      "        -0.0647,  0.0599, -0.0705, -0.1741,  0.0965,  0.1675,  0.1651, -0.0028,\n",
      "        -0.0441,  0.1003, -0.0946, -0.0208,  0.1602,  0.1275, -0.0471,  0.0049,\n",
      "        -0.1266, -0.0182,  0.0412, -0.0492, -0.1377, -0.1493,  0.1347,  0.0140],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0287, -0.0486,  0.0614, -0.0347,  0.0045,  0.0371,  0.1008, -0.1263,\n",
      "         0.0139, -0.1258,  0.1750, -0.0749, -0.0382,  0.0581, -0.0331, -0.0397,\n",
      "         0.1752,  0.1175,  0.0571, -0.1188,  0.1193,  0.1539, -0.1696, -0.1268,\n",
      "         0.0227,  0.0538, -0.0654,  0.1270, -0.1294,  0.1363,  0.0558, -0.0669],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0448,  0.1592, -0.0071, -0.1407,  0.0634, -0.1000,  0.0102, -0.0248,\n",
      "         0.1026,  0.1275,  0.1255, -0.1378,  0.0099,  0.0105,  0.0298,  0.0843,\n",
      "        -0.1165,  0.0462, -0.0727, -0.0808, -0.0456, -0.0541, -0.1252, -0.1007,\n",
      "         0.0075, -0.0570,  0.0632,  0.1152,  0.1288, -0.0302, -0.0326,  0.1690],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0697,  0.0614, -0.0245,  0.0693,  0.0958,  0.0092, -0.1557, -0.1499,\n",
      "         0.1319,  0.0531, -0.0350,  0.1667, -0.1744, -0.1233,  0.1205, -0.0864,\n",
      "         0.0414, -0.0629,  0.0043, -0.0038,  0.1566, -0.1006,  0.0464,  0.1163,\n",
      "        -0.1077,  0.0177,  0.0952,  0.1454,  0.0729, -0.1576,  0.0333, -0.1601],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1052, -0.1740,  0.1476,  0.0987, -0.0541, -0.0005, -0.1491,  0.0176,\n",
      "         0.0859,  0.0969,  0.1473, -0.0282,  0.0088, -0.1530, -0.1057,  0.0955,\n",
      "        -0.1265, -0.0877,  0.1398,  0.0050,  0.0119,  0.0199, -0.1370, -0.1265,\n",
      "        -0.1556,  0.0152,  0.1632,  0.1708,  0.1418,  0.0328,  0.1394, -0.0392],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0540,  0.1565, -0.1451, -0.0792, -0.1695, -0.1229, -0.0801, -0.0358,\n",
      "        -0.0386,  0.1141, -0.0324, -0.0705, -0.1698, -0.0634, -0.1253, -0.0114,\n",
      "         0.1611,  0.1521,  0.0951, -0.0895, -0.0040,  0.0965,  0.1085,  0.1650,\n",
      "         0.0263, -0.1652,  0.1456,  0.0610,  0.1020,  0.1341, -0.1073,  0.0332],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0873, -0.0991,  0.0032, -0.0537,  0.0542,  0.1552, -0.0566, -0.0641,\n",
      "        -0.0425,  0.1419,  0.0184, -0.1245, -0.1702,  0.1451, -0.0400,  0.1049,\n",
      "         0.1729,  0.1511, -0.0712, -0.0156,  0.0388, -0.0150,  0.1322, -0.0168,\n",
      "         0.1524, -0.0230,  0.0862, -0.1199, -0.0372,  0.1218,  0.0987,  0.0666],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0270,  0.1045,  0.0520, -0.0664, -0.1356, -0.1479,  0.0181, -0.1200,\n",
      "        -0.0231,  0.0008,  0.0669, -0.0778, -0.0597, -0.0068, -0.1326, -0.0747,\n",
      "         0.0192,  0.0320, -0.0756,  0.1185,  0.0517,  0.0926,  0.0313,  0.1587,\n",
      "         0.1231, -0.1499, -0.1751, -0.0608, -0.0220, -0.1178, -0.1390, -0.1223],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0318,  0.1764, -0.1688, -0.1226,  0.0029,  0.0794,  0.0542,  0.1129,\n",
      "        -0.0851, -0.1704, -0.1155,  0.1314, -0.0325,  0.1546, -0.0887, -0.0961,\n",
      "        -0.1729, -0.0059, -0.1606,  0.0842, -0.0985, -0.1658, -0.1025, -0.0134,\n",
      "        -0.0425, -0.1370, -0.1367,  0.0173,  0.0493,  0.0885, -0.0753, -0.1525],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0393,  0.0259, -0.0516, -0.1743, -0.0776,  0.0423, -0.0250,  0.1745,\n",
      "         0.1640,  0.1229,  0.0737, -0.0038, -0.0766, -0.1025,  0.0086, -0.1292,\n",
      "         0.0303,  0.0279,  0.0172,  0.0976,  0.1423, -0.1522, -0.0201, -0.0801,\n",
      "        -0.1385, -0.0271, -0.0983,  0.1571,  0.0210,  0.1117,  0.1563, -0.1081],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1669,  0.1252,  0.0126,  0.0199, -0.0277,  0.1733,  0.0558, -0.1028,\n",
      "        -0.1657, -0.1094,  0.0354, -0.0100,  0.0915,  0.0308, -0.0678,  0.0042,\n",
      "         0.1613, -0.1531, -0.0262,  0.0769, -0.0302,  0.1174,  0.0443,  0.1633,\n",
      "        -0.0920,  0.1474, -0.0192, -0.1640, -0.1204,  0.1432,  0.0260, -0.0287],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0538, -0.0864, -0.1042, -0.0721,  0.1022, -0.1343, -0.1695, -0.1205,\n",
      "         0.0461, -0.1148,  0.0375, -0.0356,  0.1562,  0.1075,  0.0060, -0.0032,\n",
      "        -0.1457, -0.0534, -0.0457,  0.1646, -0.1019, -0.0154,  0.0396,  0.1014,\n",
      "         0.0634, -0.1309, -0.1276, -0.1610,  0.1704, -0.1137,  0.1247,  0.0397],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1404, -0.1379,  0.0825,  0.0630, -0.0952, -0.0008, -0.0182, -0.0903,\n",
      "         0.0020,  0.0900, -0.1346,  0.1483,  0.1179, -0.0879, -0.0429,  0.0429,\n",
      "         0.1600, -0.0653,  0.0168,  0.0850, -0.0317, -0.0522,  0.0091,  0.0709,\n",
      "         0.0577,  0.1111, -0.1056, -0.1187,  0.0144,  0.1344,  0.0760,  0.1075],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0209, -0.0294, -0.0819,  0.1095, -0.1232, -0.1075, -0.0817, -0.1489,\n",
      "        -0.0181,  0.1254, -0.0485, -0.1231, -0.0024,  0.0808, -0.0041, -0.0293,\n",
      "        -0.1466,  0.0466,  0.1187, -0.0929, -0.0057,  0.0734, -0.0712, -0.1127,\n",
      "         0.1190, -0.0836,  0.1491, -0.0128,  0.1123, -0.1071,  0.1016,  0.0662],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0270, -0.0054, -0.1507, -0.1295,  0.1260, -0.0697, -0.1369, -0.0656,\n",
      "        -0.0440, -0.1001, -0.1048, -0.1389,  0.1588,  0.0178,  0.0168,  0.0651,\n",
      "        -0.1635,  0.1633, -0.0923,  0.0942,  0.1310,  0.1203,  0.0812, -0.0698,\n",
      "         0.1569,  0.1684,  0.1635,  0.0050, -0.1520,  0.0506,  0.1334, -0.0405],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1419,  0.1459,  0.1539,  0.0859, -0.1070,  0.0880, -0.0048, -0.1088,\n",
      "         0.0412, -0.0662, -0.1307, -0.1406, -0.0254, -0.0712, -0.1444, -0.1464,\n",
      "        -0.0107,  0.0826, -0.0633,  0.1750, -0.0232, -0.1731, -0.0240, -0.1015,\n",
      "        -0.1652,  0.1029, -0.0854, -0.1560, -0.1193,  0.0463,  0.1208,  0.1646],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0395,  0.0118, -0.0175, -0.1611,  0.0816,  0.0283,  0.0369, -0.0063,\n",
      "        -0.1536, -0.1348, -0.0319,  0.0422,  0.0487,  0.1636,  0.0255,  0.1291,\n",
      "         0.0721,  0.1656, -0.0754, -0.1173,  0.0863, -0.1526, -0.0985,  0.1346,\n",
      "        -0.0950,  0.1341, -0.1098,  0.0106, -0.0013, -0.0227,  0.0451, -0.0544],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0576,  0.0287, -0.0501,  0.1373,  0.1766, -0.0062,  0.0737, -0.1563,\n",
      "        -0.1541,  0.0976,  0.0549, -0.1551,  0.1647,  0.1309,  0.0621, -0.1155,\n",
      "        -0.1374, -0.0866, -0.0828,  0.1299, -0.0563, -0.0521,  0.0363, -0.0539,\n",
      "        -0.1098, -0.0917,  0.0863,  0.0007, -0.0984, -0.0898,  0.1367,  0.1310],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0451, -0.0412, -0.0661,  0.1290,  0.0836, -0.0075,  0.1089,  0.0722,\n",
      "        -0.0471, -0.0274, -0.0055,  0.0322, -0.0500,  0.0819, -0.0437, -0.1620,\n",
      "        -0.0789,  0.0654, -0.0674,  0.0184, -0.0129, -0.1325, -0.0720,  0.1477,\n",
      "        -0.1724, -0.0928, -0.0671, -0.1322, -0.0423, -0.0585,  0.0148, -0.1411],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Writing memory\n",
      "MLP_Model\n",
      "name linear2\n",
      "check --------\n",
      "external_inputs_dict {'linear1_input_0': tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])}\n",
      "input_names ['linear1_output_1']\n",
      "sub module name internal linear2\n",
      "item_name linear1\n",
      "item_data {'input_names': ['linear1_input_0'], 'input_indices': [0], 'output_names': ['linear1_output_1'], 'input_order': [0], 'output_order': [1], 'module_type': 'MLP_Model', 'module': MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      "), 'weights': tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]]), 'num_nodes': 3, 'out_addr': 5248}\n",
      "#############found\n",
      "Node attribute\n",
      "#############in message loaded\n",
      "item_name linear2\n",
      "item_data {'input_names': ['linear1_output_1'], 'input_indices': [0], 'output_names': ['linear2_output_1'], 'input_order': [2], 'output_order': [3], 'module_type': 'MLP_Model', 'module': MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      "), 'weights': tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]]), 'num_nodes': 3, 'out_addr': None}\n",
      "AMPLE suub model MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "layers.0.weight torch.Size([32, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]],\n",
      "       requires_grad=True)\n",
      "base_path /home/aw1223/ip/agile/hw/sim/layer_config/\n",
      "attr check None\n",
      "linear\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0710,  0.0266,  0.0160, -0.1139, -0.0237, -0.1057, -0.0273, -0.1236,\n",
      "        -0.0419,  0.1697, -0.1533,  0.1418,  0.0621, -0.0075, -0.0284, -0.0411,\n",
      "         0.1524,  0.1159,  0.1248, -0.0009,  0.1272, -0.1280, -0.0276, -0.0101,\n",
      "        -0.0500,  0.1080,  0.0413, -0.0751, -0.1315, -0.0566, -0.1146, -0.0305],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1374, -0.1612, -0.1360, -0.0023,  0.0700, -0.1648, -0.0699,  0.1196,\n",
      "         0.1320, -0.1478,  0.0159,  0.0438, -0.0671,  0.0734,  0.1720,  0.1658,\n",
      "         0.1271, -0.1263,  0.1540,  0.1156,  0.1032,  0.0627,  0.1127, -0.1300,\n",
      "         0.0115,  0.0916,  0.1612, -0.1727,  0.1635, -0.1006, -0.0068,  0.0423],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1172, -0.0600, -0.0220, -0.0375, -0.0505, -0.0622, -0.0180, -0.0435,\n",
      "         0.1742,  0.1761,  0.0179, -0.0680, -0.0287,  0.1057,  0.0596, -0.1125,\n",
      "         0.1025,  0.1516, -0.0657, -0.0092, -0.1569,  0.1329,  0.1178, -0.0376,\n",
      "        -0.1278, -0.0677,  0.1607, -0.0866, -0.1657,  0.1420,  0.1664,  0.1044],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1686, -0.0238,  0.0579,  0.1742, -0.1346,  0.1308, -0.1014,  0.1757,\n",
      "        -0.1572, -0.1468,  0.0412,  0.0107, -0.1367,  0.0081,  0.0631, -0.0540,\n",
      "         0.1551, -0.0286, -0.1376,  0.0586,  0.1332,  0.0317,  0.0703, -0.0947,\n",
      "         0.0315,  0.0808,  0.0251,  0.1422,  0.0560, -0.1191, -0.1219,  0.1679],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0599, -0.1578, -0.0304,  0.0713, -0.1348, -0.0485,  0.0331, -0.0245,\n",
      "         0.1013, -0.1504,  0.0163,  0.0272, -0.1487,  0.1359, -0.1178, -0.1038,\n",
      "        -0.0720,  0.0600,  0.0986, -0.0112, -0.0766,  0.0313,  0.1400,  0.1545,\n",
      "         0.0097,  0.1473, -0.0230,  0.1136,  0.0197, -0.0028, -0.0662,  0.0134],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0379, -0.1436, -0.1416, -0.0931, -0.0561,  0.0907, -0.1524, -0.0635,\n",
      "        -0.0325, -0.1180, -0.0954, -0.1584,  0.1380, -0.0153,  0.0789, -0.0763,\n",
      "         0.1637,  0.0142, -0.1196, -0.0373, -0.1742,  0.1001,  0.1082, -0.0783,\n",
      "         0.1659, -0.0918,  0.0965,  0.0510, -0.0305, -0.1150,  0.0951, -0.0691],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1632,  0.0913,  0.0231,  0.1167, -0.1431, -0.1363, -0.0385, -0.1730,\n",
      "        -0.0241,  0.1295, -0.1579, -0.0690,  0.0066,  0.0120, -0.0529,  0.0295,\n",
      "         0.1074,  0.1693, -0.1463, -0.1628,  0.0108, -0.1197, -0.1353, -0.0965,\n",
      "        -0.1058,  0.1145, -0.1335,  0.0488, -0.0115,  0.1614, -0.0493, -0.0665],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0279,  0.0225, -0.0163,  0.0981,  0.0517, -0.0546, -0.1119, -0.0439,\n",
      "         0.0545,  0.1726,  0.1349,  0.1020, -0.0010, -0.0410,  0.0188, -0.0390,\n",
      "         0.0987, -0.1229, -0.0919,  0.0400,  0.0768,  0.0290, -0.0629, -0.0189,\n",
      "        -0.0851, -0.1405, -0.1281, -0.0294, -0.1562, -0.1101, -0.1406,  0.0855],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0339,  0.1320, -0.0762, -0.0559,  0.0915, -0.1126,  0.0117,  0.0222,\n",
      "         0.0794,  0.0470,  0.1573, -0.1702, -0.1205, -0.1584, -0.1614,  0.1274,\n",
      "         0.0979, -0.0887, -0.1150, -0.0861, -0.1623,  0.1624,  0.1201, -0.0703,\n",
      "         0.0032, -0.0342, -0.0931,  0.0512,  0.0418, -0.0475,  0.0038, -0.0588],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0915,  0.0896,  0.1139,  0.1132,  0.1196,  0.0278,  0.0714, -0.1066,\n",
      "         0.0083,  0.0483,  0.1304,  0.0150, -0.1562,  0.1745,  0.0528,  0.1643,\n",
      "         0.0417, -0.1368,  0.0400, -0.0074, -0.1582,  0.1087, -0.0221, -0.1521,\n",
      "         0.1432, -0.0299, -0.0938, -0.1661, -0.1104,  0.1045, -0.0531, -0.0922],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0109,  0.0595, -0.1539, -0.1607, -0.1037,  0.1535, -0.0473,  0.1117,\n",
      "        -0.1684, -0.1194,  0.0604, -0.0106, -0.1219,  0.1548, -0.1602,  0.0781,\n",
      "        -0.1672,  0.1528, -0.0630, -0.1010, -0.0114, -0.0922, -0.0067, -0.0386,\n",
      "         0.1274,  0.1090, -0.0882, -0.1114,  0.0257, -0.0750,  0.1156,  0.1291],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0381, -0.0022, -0.1011,  0.0227,  0.1691,  0.1049,  0.1486,  0.0458,\n",
      "        -0.0102,  0.1209,  0.1594,  0.1066,  0.0886,  0.1692, -0.0065,  0.0190,\n",
      "         0.0729,  0.0941,  0.0274, -0.1509, -0.0743, -0.0895,  0.1585, -0.1515,\n",
      "        -0.1372,  0.0946, -0.0078,  0.0308, -0.0078,  0.1253,  0.0399, -0.1365],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0348,  0.0255, -0.1417,  0.0484,  0.0756,  0.1183,  0.0640, -0.1766,\n",
      "         0.1714,  0.0494,  0.1532, -0.1350,  0.1484, -0.0433,  0.1093, -0.0919,\n",
      "         0.0720, -0.0087, -0.1058,  0.1157, -0.0794, -0.0108, -0.0246,  0.0983,\n",
      "         0.1363,  0.0854,  0.1233,  0.1264, -0.1273,  0.0988, -0.1687,  0.0829],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1512, -0.0336, -0.0879, -0.0944, -0.0541,  0.0226, -0.0644,  0.1383,\n",
      "         0.0471, -0.1229,  0.0261,  0.1502, -0.1273,  0.0783,  0.1524,  0.1340,\n",
      "        -0.1342,  0.0398,  0.1042, -0.0773,  0.0603,  0.0033,  0.0410, -0.0548,\n",
      "         0.0998,  0.1765, -0.0684,  0.1680,  0.0240,  0.1741,  0.0252, -0.0636],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0080,  0.1515, -0.0757,  0.1483, -0.0542,  0.0394, -0.1716,  0.0015,\n",
      "         0.1008, -0.0466, -0.0836,  0.0074,  0.0577,  0.0576,  0.0173,  0.0244,\n",
      "        -0.0088, -0.0966, -0.1359,  0.1393,  0.0175, -0.0511, -0.1700, -0.1506,\n",
      "         0.1236,  0.0569, -0.0158, -0.1094,  0.0740,  0.1632,  0.1027, -0.0645],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0894,  0.0381,  0.0854,  0.1612, -0.0314, -0.1068, -0.1570,  0.1398,\n",
      "         0.0327, -0.0591,  0.1579,  0.1081,  0.0158, -0.0270, -0.1745, -0.1020,\n",
      "        -0.0245, -0.0733,  0.0119,  0.0326,  0.0108,  0.1132,  0.0473,  0.0825,\n",
      "         0.1654,  0.1188,  0.1510,  0.1005, -0.0911, -0.0231, -0.1032,  0.0327],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1032,  0.0790, -0.0893,  0.0779, -0.1129,  0.1698, -0.1185,  0.1261,\n",
      "         0.1444,  0.0554,  0.0371, -0.1396,  0.1567, -0.1349,  0.0608,  0.1013,\n",
      "         0.0783, -0.0548,  0.0949, -0.1348,  0.0227,  0.1103, -0.0146,  0.1028,\n",
      "        -0.0199, -0.0140,  0.1550, -0.0493,  0.0912, -0.0587,  0.1316, -0.1261],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1577, -0.0903,  0.1460, -0.1020, -0.0569,  0.1525, -0.1484, -0.1335,\n",
      "        -0.0367,  0.0499,  0.0169,  0.1059, -0.0219,  0.0278, -0.0908,  0.1107,\n",
      "         0.0282, -0.1622, -0.1256, -0.0568,  0.1289, -0.1674,  0.1268, -0.1007,\n",
      "         0.0892,  0.1174, -0.1167, -0.1078,  0.1567, -0.0692, -0.0120, -0.1562],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0289,  0.0256,  0.1405, -0.1479,  0.0420,  0.1186,  0.1412,  0.1591,\n",
      "         0.1709, -0.0766,  0.1002, -0.0614, -0.0932,  0.1370,  0.1105,  0.0706,\n",
      "        -0.0116, -0.1545, -0.0730,  0.0530, -0.1429, -0.0403,  0.0795, -0.0408,\n",
      "        -0.1487, -0.1015,  0.1038,  0.0316, -0.1321, -0.1388, -0.1083,  0.1765],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1198, -0.1594,  0.1579,  0.0457,  0.0317, -0.1464, -0.0346, -0.0506,\n",
      "        -0.0765,  0.0701,  0.1361,  0.1605,  0.1215, -0.1716, -0.0649,  0.0394,\n",
      "         0.0408,  0.1664, -0.1738,  0.1639,  0.1080,  0.1660,  0.0181, -0.0997,\n",
      "         0.0364,  0.0807, -0.1162,  0.1711,  0.0753, -0.0877, -0.0704,  0.1244],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1614, -0.0569, -0.0080,  0.0341,  0.1257,  0.0382, -0.1570, -0.0053,\n",
      "         0.1605,  0.1050,  0.0374,  0.1078, -0.1136,  0.0264, -0.0925, -0.0775,\n",
      "         0.1182,  0.0079, -0.0313,  0.1759, -0.1605, -0.0069, -0.0926,  0.1119,\n",
      "         0.1763, -0.0606,  0.0620,  0.1722, -0.1539,  0.0860,  0.1486,  0.1081],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0106, -0.0028, -0.1767,  0.1088, -0.1436, -0.0028,  0.1577,  0.1307,\n",
      "         0.1438,  0.0452,  0.0208,  0.0876, -0.1761, -0.0829,  0.1040, -0.0625,\n",
      "         0.0755,  0.1460, -0.0456,  0.0275,  0.1363,  0.0522,  0.0815, -0.1252,\n",
      "        -0.1296, -0.0538, -0.1445,  0.0105,  0.1329,  0.0374, -0.0906, -0.0319],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1447,  0.0077,  0.0291,  0.1519, -0.1037, -0.1335,  0.0990,  0.0347,\n",
      "        -0.0260, -0.0746,  0.1351, -0.0039,  0.0877, -0.0040,  0.1144,  0.0472,\n",
      "         0.0689,  0.1489,  0.0022, -0.1290,  0.1128, -0.0378, -0.0559,  0.0133,\n",
      "        -0.1355, -0.0327, -0.0104, -0.0379,  0.1076,  0.1485, -0.0989,  0.0226],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0378,  0.1277, -0.1743, -0.1524,  0.0176,  0.1205,  0.1033, -0.0808,\n",
      "         0.1443,  0.0878, -0.1468,  0.0218, -0.1557, -0.1090,  0.0516, -0.1371,\n",
      "        -0.0178, -0.0425,  0.1760,  0.1361,  0.1257, -0.0652, -0.0067, -0.1091,\n",
      "        -0.0853, -0.1137, -0.1080, -0.1369,  0.1216,  0.1259,  0.0871, -0.0082],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0712, -0.1461, -0.0064,  0.1493,  0.0710,  0.0943,  0.0273, -0.1054,\n",
      "        -0.1379, -0.0856,  0.1685, -0.0688,  0.1659, -0.0589,  0.0416,  0.1206,\n",
      "         0.0991, -0.0403, -0.0804,  0.1323,  0.1327, -0.1491,  0.1085,  0.0146,\n",
      "        -0.0624, -0.1670, -0.0115, -0.0019, -0.0693,  0.1512, -0.1301, -0.1255],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0086, -0.1678,  0.1188, -0.0960,  0.1357, -0.1074,  0.1683, -0.0464,\n",
      "         0.1505,  0.0832,  0.0188,  0.0848, -0.1727,  0.1472,  0.0079, -0.1615,\n",
      "        -0.1566, -0.0443, -0.0621, -0.1196,  0.1147, -0.1005, -0.1378, -0.0614,\n",
      "         0.0563, -0.0597,  0.0029, -0.0566,  0.0439, -0.1760,  0.0479,  0.0002],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0718,  0.0580, -0.1540, -0.1230, -0.1733, -0.0148,  0.0615, -0.1176,\n",
      "        -0.0772,  0.0735, -0.0114, -0.0250, -0.0511, -0.1009, -0.0298,  0.0559,\n",
      "         0.0335, -0.1574,  0.1338, -0.1143,  0.0297, -0.1075,  0.0791, -0.1343,\n",
      "         0.0973,  0.1062,  0.1001, -0.1438, -0.1721,  0.0557, -0.0804, -0.0349],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.0711,  0.0131, -0.0962, -0.0617, -0.1543, -0.1673,  0.0620, -0.0891,\n",
      "        -0.0449, -0.0884,  0.0123,  0.0132,  0.0205, -0.0915, -0.0851,  0.0818,\n",
      "        -0.0161, -0.0673, -0.0855, -0.1375,  0.0109, -0.1539,  0.0091,  0.1091,\n",
      "         0.0114, -0.1058, -0.0981, -0.0076,  0.0343, -0.0407, -0.0765, -0.0106],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.1158,  0.0998,  0.0702,  0.0556,  0.0308, -0.0457, -0.1341,  0.0244,\n",
      "         0.1194, -0.1718, -0.1390,  0.0064,  0.0061, -0.0998,  0.0658,  0.1426,\n",
      "        -0.0907, -0.0254,  0.0603,  0.1409, -0.1607, -0.0121,  0.0270,  0.0345,\n",
      "         0.0178,  0.1139, -0.0552,  0.1322, -0.0995,  0.1386, -0.1652, -0.1262],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([-0.0066,  0.0116, -0.1186, -0.1233, -0.0988, -0.0456, -0.1647,  0.1417,\n",
      "         0.1426,  0.1328, -0.1129, -0.1245,  0.0967,  0.1145, -0.0892, -0.0535,\n",
      "         0.0654, -0.1559,  0.1128, -0.0935, -0.1448, -0.1492, -0.1406, -0.0550,\n",
      "         0.1713,  0.0615, -0.1042, -0.0497, -0.0475, -0.1546, -0.1020, -0.1366],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1754,  0.1621,  0.0796, -0.0774,  0.1475,  0.1508,  0.1018, -0.0883,\n",
      "        -0.0859,  0.1465,  0.0380, -0.1443,  0.0370,  0.0533,  0.1009, -0.0805,\n",
      "        -0.0421, -0.1723, -0.0151,  0.0845, -0.0167,  0.0641, -0.0144,  0.0820,\n",
      "        -0.0978,  0.1735,  0.0271,  0.1576,  0.1039, -0.0564,  0.0948, -0.0879],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "memory mapper weight\n",
      "linear weight tensor([ 0.1100,  0.1128, -0.0487, -0.1602,  0.1081,  0.0023,  0.0468,  0.0907,\n",
      "         0.1302,  0.0856, -0.0161, -0.1289,  0.0899,  0.0354, -0.1264,  0.1380,\n",
      "        -0.1334, -0.0193,  0.1439,  0.0245, -0.1624,  0.0603,  0.0349,  0.1461,\n",
      "        -0.0071, -0.0466, -0.0114, -0.1247,  0.0386,  0.1129,  0.0227, -0.0095],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 46192.78it/s]\n",
      "10it [00:00, 59074.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated configuration successfully written back to file.\n",
      "saving ToyModel(\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x MLP_Model(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear1): MLP_Model(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (linear2): MLP_Model(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 1\n",
      "[tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "          0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "         -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "         -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "        [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "          0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "         -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "          0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "        [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "         -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "         -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "         -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]],\n",
      "       grad_fn=<MmBackward0>)]\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 2\n",
      "[tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "         -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "         -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "         -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "         -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "          2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "         -4.3428e-01,  1.1919e-01],\n",
      "        [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "         -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "         -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "          2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "          3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "          5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "          4.9247e-01,  1.5074e-01],\n",
      "        [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "         -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "          3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "         -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "          8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "         -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "         -6.7968e-01, -1.9930e-01]], grad_fn=<MmBackward0>)]\n",
      "tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 1\n",
      "[tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "          0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "         -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "         -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "        [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "          0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "         -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "          0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "        [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "         -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "         -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "         -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]])]\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 2\n",
      "[tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "         -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "         -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "         -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "         -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "          2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "         -4.3428e-01,  1.1919e-01],\n",
      "        [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "         -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "         -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "          2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "          3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "          5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "          4.9247e-01,  1.5074e-01],\n",
      "        [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "         -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "          3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "         -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "          8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "         -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "         -6.7968e-01, -1.9930e-01]])]\n",
      "tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 1\n",
      "[tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "          0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "         -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "         -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "        [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "          0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "         -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "          0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "        [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "         -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "         -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "         -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]])]\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 2\n",
      "[tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "         -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "         -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "         -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "         -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "          2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "         -4.3428e-01,  1.1919e-01],\n",
      "        [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "         -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "         -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "          2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "          3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "          5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "          4.9247e-01,  1.5074e-01],\n",
      "        [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "         -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "          3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "         -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "          8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "         -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "         -6.7968e-01, -1.9930e-01]])]\n",
      "jit model saving to /home/aw1223/ip/agile/hw/sim/layer_config/model.pt\n",
      "saved inputs [tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])]\n",
      "Model and graph saved\n",
      "Overloading\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "print('weights')\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pwd\n",
    "ample = Ample()\n",
    "ample.sim = True\n",
    "#Need weights to be initialized before calling to_device\n",
    "# model = GraphLam_Model(<parameters>)\n",
    "model.to_device('ample',data=inputs) #Change \n",
    "\n",
    "# out = model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing on AMPLE\n",
      "cd /home/aw1223/ip/agile/hw/sim\n",
      "==== Running command: cd /home/aw1223/ip/agile/hw/sim; make run_sim\n",
      "make -f Makefile.modelsim run_sim\n",
      "make[1]: Entering directory '/home/aw1223/ip/agile/hw/sim'\n",
      "set -o pipefail; LIBPYTHON_LOC=/home/aw1223/anaconda3/envs/ample/lib/libpython3.11.so.1.0 MODULE=runner TESTCASE= TOPLEVEL=\"work.top_wrapper\" \\\n",
      "GPI_EXTRA= TOPLEVEL_LANG=verilog \\\n",
      " /mnt/applications/mentor/modelsim-2019.2/modelsim/modeltech/linux_x86_64/vsim -c -64 -l sim_build/modelsim.log -onfinish exit -do sim_build/../simulate_no_gui.do -quiet -suppress vsim-3015  -pli /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libcocotbvpi_modelsim.so \\\n",
      " \\\n",
      "work.work_opt \\\n",
      " 2>&1 | tee sim_build/simulate.log\n",
      "/mnt/applications/mentor/modelsim-2019.2/modelsim/modeltech/linux_x86_64/vish: /mnt/applications/Xilinx/19.2/Vitis/2019.2/lib/lnx64.o/libxml2.so.2: no version information available (required by /lib64/libfontconfig.so.1)\n",
      "/mnt/applications/mentor/modelsim-2019.2/modelsim/modeltech/linux_x86_64/vish: /mnt/applications/Xilinx/19.2/Vitis/2019.2/lib/lnx64.o/libxml2.so.2: no version information available (required by /lib64/libfontconfig.so.1)\n",
      "Reading pref.tcl\n",
      "\n",
      "# 2019.2\n",
      "\n",
      "# vsim -c -l sim_build/modelsim.log -onfinish exit -do \"sim_build/../simulate_no_gui.do\" -quiet -suppress vsim-3015 -pli \"/home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libcocotbvpi_modelsim.so\" work.work_opt \n",
      "# Start time: 14:26:30 on Aug 30,2024\n",
      "# //  ModelSim SE-64 2019.2 Apr 16 2019 Linux 5.14.0-427.24.1.el9_4.x86_64\n",
      "# //\n",
      "# //  Copyright 1991-2019 Mentor Graphics Corporation\n",
      "# //  All Rights Reserved.\n",
      "# //\n",
      "# //  ModelSim SE-64 and its associated documentation contain trade\n",
      "# //  secrets and commercial or financial information that are the property of\n",
      "# //  Mentor Graphics Corporation and are privileged, confidential,\n",
      "# //  and exempt from disclosure under the Freedom of Information Act,\n",
      "# //  5 U.S.C. Section 552. Furthermore, this information\n",
      "# //  is prohibited from disclosure under the Trade Secrets Act,\n",
      "# //  18 U.S.C. Section 1905.\n",
      "# //\n",
      "#      -.--ns INFO     gpi                                ..mbed/gpi_embed.cpp:79   in set_program_name_in_venv        Did not detect Python virtual environment. Using system-wide Python interpreter\n",
      "#      -.--ns INFO     gpi                                ../gpi/GpiCommon.cpp:101  in gpi_print_registered_impl       VPI registered\n",
      "# do sim_build/../simulate_no_gui.do\n",
      "# 1\n",
      "# 1\n",
      "# Attempting stack trace sig 11\n",
      "# Signal caught: signo [11]\n",
      "#      0.00ns INFO     cocotb                             Running on ModelSim SE-64 version 2019.2 2019.04\n",
      "#      0.00ns INFO     cocotb                             Running tests with cocotb v1.9.0 from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb\n",
      "#      0.00ns INFO     cocotb                             Seeding Python random module with 1725024393\n",
      "#      0.00ns INFO     cocotb.regression                  Found test runner.graph_test\n",
      "#      0.00ns INFO     cocotb.regression                  running graph_test (1/1)\n",
      "# &&&&&&&&&&tolerance --------- 0.1\n",
      "# Model name: ToyModel\n",
      "# model_name ToyModel\n",
      "#      0.00ns INFO     cocotb.top_wrapper                 ************************************************************\n",
      "#                                                         *                                                          *\n",
      "#                                                         *                   Starting Graph Test                    *\n",
      "#                                                         *                                                          *\n",
      "#                                                         ************************************************************\n",
      "# loading model from /home/aw1223/ip/agile/hw/sim/layer_config/model.pt\n",
      "# loading graph from /home/aw1223/ip/agile/hw/sim/layer_config/graph.pth\n",
      "# model RecursiveScriptModule(\n",
      "#   original_name=ToyModel\n",
      "#   (layers): RecursiveScriptModule(\n",
      "#     original_name=ModuleList\n",
      "#     (0): RecursiveScriptModule(\n",
      "#       original_name=MLP_Model\n",
      "#       (layers): RecursiveScriptModule(\n",
      "#         original_name=ModuleList\n",
      "#         (0): RecursiveScriptModule(original_name=Linear)\n",
      "#       )\n",
      "#     )\n",
      "#     (1): RecursiveScriptModule(\n",
      "#       original_name=MLP_Model\n",
      "#       (layers): RecursiveScriptModule(\n",
      "#         original_name=ModuleList\n",
      "#         (0): RecursiveScriptModule(original_name=Linear)\n",
      "#       )\n",
      "#     )\n",
      "#   )\n",
      "#   (linear1): RecursiveScriptModule(\n",
      "#     original_name=MLP_Model\n",
      "#     (layers): RecursiveScriptModule(\n",
      "#       original_name=ModuleList\n",
      "#       (0): RecursiveScriptModule(original_name=Linear)\n",
      "#     )\n",
      "#   )\n",
      "#   (linear2): RecursiveScriptModule(\n",
      "#     original_name=MLP_Model\n",
      "#     (layers): RecursiveScriptModule(\n",
      "#       original_name=ModuleList\n",
      "#       (0): RecursiveScriptModule(original_name=Linear)\n",
      "#     )\n",
      "#   )\n",
      "# )\n",
      "# Inputs: [tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "#          -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "#           0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "#          -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "#         [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "#          -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "#          -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "#           2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "#         [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "#           4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "#           4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "#           5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])]\n",
      "# model\n",
      "# layers.0.layers.0.weight tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "#         [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "#         [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "#         ...,\n",
      "#         [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "#         [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "#         [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]]) name,param\n",
      "# model\n",
      "# layers.1.layers.0.weight tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "#         [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "#         [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "#         ...,\n",
      "#         [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "#         [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "#         [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]]) name,param\n",
      "# model\n",
      "# linear1.layers.0.weight tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "#         [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "#         [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "#         ...,\n",
      "#         [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "#         [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "#         [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]]) name,param\n",
      "# model\n",
      "# linear2.layers.0.weight tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "#         [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "#         [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "#         ...,\n",
      "#         [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "#         [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "#         [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]]) name,param\n",
      "# Output: [tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "#           0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "#          -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "#          -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "#         [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "#           0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "#          -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "#           0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "#         [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "#          -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "#          -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "#          -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]]), tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "#          -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "#          -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "#          -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "#          -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "#           2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "#          -4.3428e-01,  1.1919e-01],\n",
      "#         [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "#          -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "#          -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "#           2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "#           3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "#           5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "#           4.9247e-01,  1.5074e-01],\n",
      "#         [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "#          -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "#           3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "#          -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "#           8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "#          -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "#          -6.7968e-01, -1.9930e-01]])]\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Input Tensors:\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading nodeslot programming\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading layer configuration\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading register banks.\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading node_scoreboard_regbank from /home/aw1223/ip/agile/hw/build/regbanks/node_scoreboard_regbank/node_scoreboard_regbank_regs.json\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading prefetcher_regbank from /home/aw1223/ip/agile/hw/build/regbanks/prefetcher_regbank/prefetcher_regbank_regs.json\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading aggregation_engine_regbank from /home/aw1223/ip/agile/hw/build/regbanks/aggregation_engine_regbank/aggregation_engine_regbank_regs.json\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Loading feature_transformation_engine_regbank from /home/aw1223/ip/agile/hw/build/regbanks/feature_transformation_engine_regbank/feature_transformation_engine_regbank_regs.json\n",
      "#      0.00ns DEBUG    cocotb.top_wrapper                 Driving reset\n",
      "# Addressing configuration for axil_interconnect instance top_wrapper.top_i.axil_interconnect_i.axil_interconnect_inst\n",
      "#  0 ( 0): 00000000 / 30 -- 00000000-3fffffff\n",
      "#  1 ( 0): 40000000 / 30 -- 40000000-7fffffff\n",
      "#  2 ( 0): 80000000 / 30 -- 80000000-bfffffff\n",
      "#  3 ( 0): c0000000 / 30 -- c0000000-ffffffff\n",
      "# Addressing configuration for axi_interconnect instance top_wrapper.top_i.memory_interconnect.axi_interconnect_inst\n",
      "#  0 ( 0): 000000000 / 24 -- 000000000-000ffffff\n",
      "# Addressing configuration for axi_interconnect instance top_wrapper.xdma_write_interconnect_i.axi_interconnect_inst\n",
      "#  0 ( 0): 000000000 / 24 -- 000000000-000ffffff\n",
      "#  1 ( 0): 001000000 / 24 -- 001000000-001ffffff\n",
      "# Addressing configuration for axi_interconnect instance top_wrapper.ample_xdma_mem_interconnect_i.axi_interconnect_inst\n",
      "#  0 ( 0): 000000000 / 24 -- 000000000-000ffffff\n",
      "#    245.00ns DEBUG    cocotb.top_wrapper                 Reset done\n",
      "#    245.00ns DEBUG    cocotb.top_wrapper                 Starting wait after reset\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Done waiting after reset\n",
      "#    295.00ns INFO     cocotb.top_wrapper                 Graph initialized with log level: 10\n",
      "# 2\n",
      "# 6a\n",
      "# 7a\n",
      "#    295.00ns INFO     cocotb.top_wrapper                 Starting layer 0\n",
      "# 8a\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Layer Out Expected tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "#                                                                   0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "#                                                                  -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "#                                                                  -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "#                                                                 [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "#                                                                   0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "#                                                                  -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "#                                                                   0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "#                                                                 [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "#                                                                  -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "#                                                                  -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "#                                                                  -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]])\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Loading expected nodes into monitor\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Loading Layer Features\n",
      "# nodeslots [{'node_id': 0, 'neighbour_count': 1, 'precision': 'FLOAT_32', 'aggregation_function': 'SUM', 'adjacency_list_address_msb': 0, 'scale_factors_address_lsb': 576, 'scale_factors_address_msb': 0, 'out_messages_address_lsb': 0, 'out_messages_address_msb': 0}, {'node_id': 1, 'neighbour_count': 1, 'precision': 'FLOAT_32', 'aggregation_function': 'SUM', 'adjacency_list_address_msb': 0, 'scale_factors_address_lsb': 640, 'scale_factors_address_msb': 0, 'out_messages_address_lsb': 128, 'out_messages_address_msb': 0}, {'node_id': 2, 'neighbour_count': 1, 'precision': 'FLOAT_32', 'aggregation_function': 'SUM', 'adjacency_list_address_msb': 0, 'scale_factors_address_lsb': 704, 'scale_factors_address_msb': 0, 'out_messages_address_lsb': 256, 'out_messages_address_msb': 0}]\n",
      "# node_id 0\n",
      "# node_id 1\n",
      "# node_id 2\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Expected Data Indexed by Address:\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Address: 5248, Node: 0\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Data: tensor([-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "#                                                                  0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "#                                                                 -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "#                                                                 -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167])\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Address: 5376, Node: 1\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Data: tensor([-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "#                                                                  0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "#                                                                 -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "#                                                                  0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064])\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Address: 5504, Node: 2\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Data: tensor([-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "#                                                                 -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "#                                                                 -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "#                                                                 -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519])\n",
      "# 9a\n",
      "# nsb_regs {'CTRL_FETCH_LAYER_WEIGHTS': 2147483648, 'CTRL_FETCH_LAYER_WEIGHTS_DONE': 2147483652, 'CTRL_FETCH_LAYER_WEIGHTS_DONE_ACK': 2147483656, 'nsb_nodeslot_neighbour_count': 2147483676, 'nsb_nodeslot_node_id': 2147484700, 'nsb_nodeslot_node_state': 2147485724, 'nsb_nodeslot_precision': 2147486748, 'NSB_CONFIG_AGGREGATION_WAIT_COUNT': 2147491868, 'nsb_nodeslot_allocated_fetch_tag': 2147491876, 'layer_config_in_features': 2147492900, 'layer_config_out_features': 2147492904, 'layer_config_adjacency_list_address_lsb': 2147492908, 'layer_config_adjacency_list_address_msb': 2147492912, 'nsb_nodeslot_aggregation_function': 2147492916, 'ctrl_fetch_layer_weights_precision': 2147495988, 'layer_config_valid': 2147495992, 'layer_config_weights_address_lsb': 2147495996, 'layer_config_weights_address_msb': 2147496012, 'nsb_nodeslot_config_make_valid': 2147496028, 'status_nodeslots_empty_mask_0': 2147497052, 'status_nodeslots_empty_mask_1': 2147497056, 'status_nodeslots_empty_mask_2': 2147497060, 'status_nodeslots_empty_mask_3': 2147497064, 'status_nodeslots_empty_mask_4': 2147497068, 'status_nodeslots_empty_mask_5': 2147497072, 'status_nodeslots_empty_mask_6': 2147497076, 'status_nodeslots_empty_mask_7': 2147497080, 'status_nodeslots_empty_mask_lsb': 2147497084, 'status_nodeslots_empty_mask_msb': 2147497088, 'layer_config_scale_factors_address_lsb': 2147497092, 'layer_config_scale_factors_address_msb': 2147497096, 'graph_config_node_count': 2147497100, 'ctrl_start_nodeslot_fetch': 2147497104, 'ctrl_start_nodeslot_fetch_done': 2147497108, 'ctrl_start_nodeslot_fetch_done_ack': 2147497112, 'layer_config_aggregate_enable': 2147497116, 'ctrl_start_nodeslot_fetch_start_addr': 2147497120, 'concat_width': 2147497124, 'status_leds': 2147497128}\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Ready to program layer configuration\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Layer: {'name': 'mlp_input_output_layer', 'nodeslot_count': 3, 'in_feature_count': 32, 'out_feature_count': 32, 'transformation_activation': 0, 'leaky_relu_alpha': 0, 'transformation_bias': 0, 'dequantization_parameter': 1, 'adjacency_list_address': 192, 'in_messages_address': 768, 'weights_address': 1152, 'out_messages_address': 5248, 'aggregation_wait_count': 16, 'transformation_wait_count': 16, 'aggregate_enable': 0, 'edge_node': 0, 'nodeslot_start_address': 0, 'concat_width': 1}\n",
      "#    295.00ns DEBUG    cocotb.top_wrapper                 Programming prefetcher register bank layer configuration.\n",
      "#    520.00ns DEBUG    cocotb.top_wrapper                 Programming AGE register bank layer configuration.\n",
      "#    610.00ns DEBUG    cocotb.top_wrapper                 Programming FTE register bank layer configuration.\n",
      "#    745.00ns DEBUG    cocotb.top_wrapper                 Programming NSB register bank layer configuration.\n",
      "# 10a\n",
      "#   1060.00ns DEBUG    cocotb.top_wrapper                 Requesting weights fetch for precision FLOAT_32.\n",
      "#   8700.00ns DEBUG    cocotb.top_wrapper                 2147483652 register is asserted\n",
      "#   8745.00ns DEBUG    cocotb.top_wrapper                 Weights fetch done.\n",
      "# 11a\n",
      "# 12a\n",
      "# 13a\n",
      "#   8975.00ns DEBUG    cocotb.top_wrapper                 2147497108 register is asserted\n",
      "# 14a\n",
      "#   9020.00ns DEBUG    cocotb.top_wrapper                 Nodeslot fetching done, waiting for nodeslots to be flushed.\n",
      "# 15a\n",
      "#   9020.00ns DEBUG    cocotb.top_wrapper                 Waiting for nodeslots to be empty.\n",
      "#   9050.00ns DEBUG    cocotb.top_wrapper                 Observed 1 response to NSB for Nodeslot: 00000\n",
      "#   9055.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 3 for Nodeslot 0\n",
      "#   9120.00ns DEBUG    cocotb.top_wrapper                 Observed 1 response to NSB for Nodeslot: 00001\n",
      "#   9125.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 3 for Nodeslot 1\n",
      "#   9190.00ns DEBUG    cocotb.top_wrapper                 Observed 3 response to NSB for Nodeslot: 00000\n",
      "#   9195.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 2 for Nodeslot 0\n",
      "#   9260.00ns DEBUG    cocotb.top_wrapper                 Observed 3 response to NSB for Nodeslot: 00001\n",
      "#   9265.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 2 for Nodeslot 1\n",
      "#   9325.00ns DEBUG    cocotb.top_wrapper                 Observed 2 response to NSB for Nodeslot: 00000\n",
      "#   9330.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to AGE for Nodeslot 0\n",
      "#   9370.00ns DEBUG    cocotb.top_wrapper                 Observed Message Channel request for Nodeslot: 0, Fetch Tag: 0\n",
      "#   9400.00ns DEBUG    cocotb.top_wrapper                 Observed 2 response to NSB for Nodeslot: 00001\n",
      "#   9405.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to AGE for Nodeslot 1\n",
      "#   9430.00ns DEBUG    cocotb.top_wrapper                 Observed scale factor pop for fetch tag 0 with data: 00000000000000000000000000000000\n",
      "#   9445.00ns DEBUG    cocotb.top_wrapper                 Observed Message Channel request for Nodeslot: 1, Fetch Tag: 1\n",
      "#   9480.00ns DEBUG    cocotb.top_wrapper                 Observed 1 response to NSB for Nodeslot: 00010\n",
      "#   9485.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 3 for Nodeslot 2\n",
      "#   9505.00ns DEBUG    cocotb.top_wrapper                 Observed scale factor pop for fetch tag 1 with data: 00000000000000000000000000000000\n",
      "#   9545.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 0\n",
      "#   9570.00ns DEBUG    cocotb.top_wrapper                 Observed 3 response to NSB for Nodeslot: 00010\n",
      "#   9575.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 2 for Nodeslot 2\n",
      "#   9595.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 0\n",
      "#   9620.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 1\n",
      "#   9655.00ns DEBUG    cocotb.top_wrapper                 Observed 2 response to NSB for Nodeslot: 00010\n",
      "#   9660.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to AGE for Nodeslot 2\n",
      "#   9670.00ns DEBUG    cocotb.top_wrapper                 Observed AGE response to NSB for Nodeslot: 0\n",
      "#   9670.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 1\n",
      "#   9700.00ns DEBUG    cocotb.top_wrapper                 Observed Message Channel request for Nodeslot: 2, Fetch Tag: 2\n",
      "#   9745.00ns DEBUG    cocotb.top_wrapper                 Observed AGE response to NSB for Nodeslot: 1\n",
      "#   9760.00ns DEBUG    cocotb.top_wrapper                 Observed scale factor pop for fetch tag 2 with data: 00000000000000000000000000000000\n",
      "#   9875.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 2\n",
      "#   9925.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 2\n",
      "#  10000.00ns DEBUG    cocotb.top_wrapper                 Observed AGE response to NSB for Nodeslot: 2\n",
      "#  25850.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to FTE for Nodeslots 00000000000000000000000000000111\n",
      "#  25855.00ns DEBUG    cocotb.top_wrapper                 Observed Weight Channel request out features: 10000000000, in features 10000000000  \n",
      "#  26915.00ns DEBUG    cocotb.top_wrapper                 Transaction {'start_address': 5248, 'data': [], 'expected_length': 2}\n",
      "# axi4 \n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 Getting node\n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 --------------------\n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 \n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 Node found: 0, Address: 5248\n",
      "# axi5 \n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 Data expected tensor([-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "#                                                                  0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "#                                                                 -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "#                                                                 -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167])\n",
      "# axi6 \n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 Data gotten tensor([-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "#                                                                  0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "#                                                                 -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "#                                                                 -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167])\n",
      "# axi7 \n",
      "# toelrancer  0.1\n",
      "# axi8 \n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 Data and address correctly matched for node: 0\n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                  \n",
      "#  26985.00ns DEBUG    cocotb.top_wrapper                 --------------------\n",
      "# axi9 \n",
      "# axi7 \n",
      "#  26995.00ns DEBUG    cocotb.top_wrapper                 Transaction {'start_address': 5376, 'data': [], 'expected_length': 2}\n",
      "# axi4 \n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 Getting node\n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 --------------------\n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 \n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 Node found: 1, Address: 5376\n",
      "# axi5 \n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 Data expected tensor([-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "#                                                                  0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "#                                                                 -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "#                                                                  0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064])\n",
      "# axi6 \n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 Data gotten tensor([-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "#                                                                  0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "#                                                                 -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "#                                                                  0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064])\n",
      "# axi7 \n",
      "# toelrancer  0.1\n",
      "# axi8 \n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 Data and address correctly matched for node: 1\n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                  \n",
      "#  27065.00ns DEBUG    cocotb.top_wrapper                 --------------------\n",
      "# axi9 \n",
      "# axi7 \n",
      "#  27075.00ns DEBUG    cocotb.top_wrapper                 Transaction {'start_address': 5504, 'data': [], 'expected_length': 2}\n",
      "# axi4 \n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 Getting node\n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 --------------------\n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 \n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 Node found: 2, Address: 5504\n",
      "# axi5 \n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 Data expected tensor([-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "#                                                                 -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "#                                                                 -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "#                                                                 -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519])\n",
      "# axi6 \n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 Data gotten tensor([-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "#                                                                 -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "#                                                                 -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "#                                                                 -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519])\n",
      "# axi7 \n",
      "# toelrancer  0.1\n",
      "# axi8 \n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 Data and address correctly matched for node: 2\n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                  \n",
      "#  27145.00ns DEBUG    cocotb.top_wrapper                 --------------------\n",
      "# axi9 \n",
      "# axi7 \n",
      "#  27150.00ns DEBUG    cocotb.top_wrapper                 Observed response to NSB for nodeslots: 00000000000000000000000000000111, precision: 0\n",
      "# 16a\n",
      "# 1\n",
      "# 2\n",
      "#  27170.00ns INFO     cocotb.top_wrapper                 All nodes written.\n",
      "# 3\n",
      "#  27170.00ns INFO     cocotb.top_wrapper                 Layer 0 finished.\n",
      "# 4\n",
      "# 6a\n",
      "# 7a\n",
      "#  27220.00ns INFO     cocotb.top_wrapper                 Starting layer 1\n",
      "# 8a\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Layer Out Expected tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "#                                                                  -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "#                                                                  -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "#                                                                  -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "#                                                                  -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "#                                                                   2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "#                                                                  -4.3428e-01,  1.1919e-01],\n",
      "#                                                                 [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "#                                                                  -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "#                                                                  -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "#                                                                   2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "#                                                                   3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "#                                                                   5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "#                                                                   4.9247e-01,  1.5074e-01],\n",
      "#                                                                 [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "#                                                                  -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "#                                                                   3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "#                                                                  -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "#                                                                   8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "#                                                                  -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "#                                                                  -6.7968e-01, -1.9930e-01]])\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Loading expected nodes into monitor\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Loading Layer Features\n",
      "# nodeslots [{'node_id': 0, 'neighbour_count': 1, 'precision': 'FLOAT_32', 'aggregation_function': 'SUM', 'adjacency_list_address_msb': 0, 'scale_factors_address_lsb': 6208, 'scale_factors_address_msb': 0, 'out_messages_address_lsb': 0, 'out_messages_address_msb': 0}, {'node_id': 1, 'neighbour_count': 1, 'precision': 'FLOAT_32', 'aggregation_function': 'SUM', 'adjacency_list_address_msb': 0, 'scale_factors_address_lsb': 6272, 'scale_factors_address_msb': 0, 'out_messages_address_lsb': 128, 'out_messages_address_msb': 0}, {'node_id': 2, 'neighbour_count': 1, 'precision': 'FLOAT_32', 'aggregation_function': 'SUM', 'adjacency_list_address_msb': 0, 'scale_factors_address_lsb': 6336, 'scale_factors_address_msb': 0, 'out_messages_address_lsb': 256, 'out_messages_address_msb': 0}]\n",
      "# node_id 0\n",
      "# node_id 1\n",
      "# node_id 2\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Expected Data Indexed by Address:\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Address: 10496, Node: 0\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Data: tensor([-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "#                                                                 -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "#                                                                 -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "#                                                                 -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "#                                                                 -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "#                                                                  2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "#                                                                 -4.3428e-01,  1.1919e-01])\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Address: 10624, Node: 1\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Data: tensor([-0.3274,  0.3359,  0.1976, -0.3546, -0.3419, -0.2879, -0.4712,  0.3698,\n",
      "#                                                                  0.2370,  0.4559, -0.4859, -0.0818,  0.3963, -0.2683,  0.5491,  0.2645,\n",
      "#                                                                 -0.0178,  0.5076,  0.0058,  0.4794,  0.3950, -0.2110, -0.3805,  0.2472,\n",
      "#                                                                 -0.1150,  0.5551, -0.1051, -0.3690, -0.2604, -0.2044,  0.4925,  0.1507])\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Address: 10752, Node: 2\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Data: tensor([ 0.2545, -1.5792, -0.5518, -0.1794,  1.0951, -0.7211, -0.8297, -1.0739,\n",
      "#                                                                 -1.0610, -1.3977,  0.3318, -0.1013, -1.6183,  0.7199, -1.4779, -0.8446,\n",
      "#                                                                 -0.7030, -0.0124,  2.1482, -2.1476,  0.8475,  0.4065, -1.2422, -0.0772,\n",
      "#                                                                  0.1496, -0.2385,  0.0430,  0.4045,  2.7452,  1.4226, -0.6797, -0.1993])\n",
      "# 9a\n",
      "# nsb_regs {'CTRL_FETCH_LAYER_WEIGHTS': 2147483648, 'CTRL_FETCH_LAYER_WEIGHTS_DONE': 2147483652, 'CTRL_FETCH_LAYER_WEIGHTS_DONE_ACK': 2147483656, 'nsb_nodeslot_neighbour_count': 2147483676, 'nsb_nodeslot_node_id': 2147484700, 'nsb_nodeslot_node_state': 2147485724, 'nsb_nodeslot_precision': 2147486748, 'NSB_CONFIG_AGGREGATION_WAIT_COUNT': 2147491868, 'nsb_nodeslot_allocated_fetch_tag': 2147491876, 'layer_config_in_features': 2147492900, 'layer_config_out_features': 2147492904, 'layer_config_adjacency_list_address_lsb': 2147492908, 'layer_config_adjacency_list_address_msb': 2147492912, 'nsb_nodeslot_aggregation_function': 2147492916, 'ctrl_fetch_layer_weights_precision': 2147495988, 'layer_config_valid': 2147495992, 'layer_config_weights_address_lsb': 2147495996, 'layer_config_weights_address_msb': 2147496012, 'nsb_nodeslot_config_make_valid': 2147496028, 'status_nodeslots_empty_mask_0': 2147497052, 'status_nodeslots_empty_mask_1': 2147497056, 'status_nodeslots_empty_mask_2': 2147497060, 'status_nodeslots_empty_mask_3': 2147497064, 'status_nodeslots_empty_mask_4': 2147497068, 'status_nodeslots_empty_mask_5': 2147497072, 'status_nodeslots_empty_mask_6': 2147497076, 'status_nodeslots_empty_mask_7': 2147497080, 'status_nodeslots_empty_mask_lsb': 2147497084, 'status_nodeslots_empty_mask_msb': 2147497088, 'layer_config_scale_factors_address_lsb': 2147497092, 'layer_config_scale_factors_address_msb': 2147497096, 'graph_config_node_count': 2147497100, 'ctrl_start_nodeslot_fetch': 2147497104, 'ctrl_start_nodeslot_fetch_done': 2147497108, 'ctrl_start_nodeslot_fetch_done_ack': 2147497112, 'layer_config_aggregate_enable': 2147497116, 'ctrl_start_nodeslot_fetch_start_addr': 2147497120, 'concat_width': 2147497124, 'status_leds': 2147497128}\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Ready to program layer configuration\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Layer: {'name': 'mlp_input_output_layer', 'nodeslot_count': 3, 'in_feature_count': 32, 'out_feature_count': 32, 'transformation_activation': 0, 'leaky_relu_alpha': 0, 'transformation_bias': 0, 'dequantization_parameter': 1, 'adjacency_list_address': 5824, 'in_messages_address': 5248, 'weights_address': 6400, 'out_messages_address': 10496, 'aggregation_wait_count': 16, 'transformation_wait_count': 16, 'aggregate_enable': 0, 'edge_node': 0, 'nodeslot_start_address': 640, 'concat_width': 1}\n",
      "#  27220.00ns DEBUG    cocotb.top_wrapper                 Programming prefetcher register bank layer configuration.\n",
      "#  27445.00ns DEBUG    cocotb.top_wrapper                 Programming AGE register bank layer configuration.\n",
      "#  27535.00ns DEBUG    cocotb.top_wrapper                 Programming FTE register bank layer configuration.\n",
      "#  27670.00ns DEBUG    cocotb.top_wrapper                 Programming NSB register bank layer configuration.\n",
      "# 10a\n",
      "#  27985.00ns DEBUG    cocotb.top_wrapper                 Requesting weights fetch for precision FLOAT_32.\n",
      "#  35625.00ns DEBUG    cocotb.top_wrapper                 2147483652 register is asserted\n",
      "#  35670.00ns DEBUG    cocotb.top_wrapper                 Weights fetch done.\n",
      "# 11a\n",
      "# 12a\n",
      "# 13a\n",
      "#  35900.00ns DEBUG    cocotb.top_wrapper                 2147497108 register is asserted\n",
      "# 14a\n",
      "#  35945.00ns DEBUG    cocotb.top_wrapper                 Nodeslot fetching done, waiting for nodeslots to be flushed.\n",
      "# 15a\n",
      "#  35945.00ns DEBUG    cocotb.top_wrapper                 Waiting for nodeslots to be empty.\n",
      "#  35975.00ns DEBUG    cocotb.top_wrapper                 Observed 1 response to NSB for Nodeslot: 00000\n",
      "#  35980.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 3 for Nodeslot 0\n",
      "#  36045.00ns DEBUG    cocotb.top_wrapper                 Observed 1 response to NSB for Nodeslot: 00001\n",
      "#  36050.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 3 for Nodeslot 1\n",
      "#  36115.00ns DEBUG    cocotb.top_wrapper                 Observed 3 response to NSB for Nodeslot: 00000\n",
      "#  36120.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 2 for Nodeslot 0\n",
      "#  36185.00ns DEBUG    cocotb.top_wrapper                 Observed 3 response to NSB for Nodeslot: 00001\n",
      "#  36190.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 2 for Nodeslot 1\n",
      "#  36250.00ns DEBUG    cocotb.top_wrapper                 Observed 2 response to NSB for Nodeslot: 00000\n",
      "#  36255.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to AGE for Nodeslot 0\n",
      "#  36295.00ns DEBUG    cocotb.top_wrapper                 Observed Message Channel request for Nodeslot: 0, Fetch Tag: 0\n",
      "#  36325.00ns DEBUG    cocotb.top_wrapper                 Observed 2 response to NSB for Nodeslot: 00001\n",
      "#  36330.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to AGE for Nodeslot 1\n",
      "#  36355.00ns DEBUG    cocotb.top_wrapper                 Observed scale factor pop for fetch tag 0 with data: 00000000000000000000000000000000\n",
      "#  36370.00ns DEBUG    cocotb.top_wrapper                 Observed Message Channel request for Nodeslot: 1, Fetch Tag: 1\n",
      "#  36405.00ns DEBUG    cocotb.top_wrapper                 Observed 1 response to NSB for Nodeslot: 00010\n",
      "#  36410.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 3 for Nodeslot 2\n",
      "#  36430.00ns DEBUG    cocotb.top_wrapper                 Observed scale factor pop for fetch tag 1 with data: 00000000000000000000000000000000\n",
      "#  36470.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 0\n",
      "#  36495.00ns DEBUG    cocotb.top_wrapper                 Observed 3 response to NSB for Nodeslot: 00010\n",
      "#  36500.00ns DEBUG    cocotb.top_wrapper                 Observed Prefetcher request: 2 for Nodeslot 2\n",
      "#  36520.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 0\n",
      "#  36545.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 1\n",
      "#  36580.00ns DEBUG    cocotb.top_wrapper                 Observed 2 response to NSB for Nodeslot: 00010\n",
      "#  36585.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to AGE for Nodeslot 2\n",
      "#  36595.00ns DEBUG    cocotb.top_wrapper                 Observed AGE response to NSB for Nodeslot: 0\n",
      "#  36595.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 1\n",
      "#  36625.00ns DEBUG    cocotb.top_wrapper                 Observed Message Channel request for Nodeslot: 2, Fetch Tag: 2\n",
      "#  36670.00ns DEBUG    cocotb.top_wrapper                 Observed AGE response to NSB for Nodeslot: 1\n",
      "#  36685.00ns DEBUG    cocotb.top_wrapper                 Observed scale factor pop for fetch tag 2 with data: 00000000000000000000000000000000\n",
      "#  36800.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 2\n",
      "#  36850.00ns DEBUG    cocotb.top_wrapper                 Observing FLOAT_32 Aggregation Buffer write to slot 2\n",
      "#  36925.00ns DEBUG    cocotb.top_wrapper                 Observed AGE response to NSB for Nodeslot: 2\n",
      "#  51455.00ns DEBUG    cocotb.top_wrapper                 Observed NSB request to FTE for Nodeslots 00000000000000000000000000000111\n",
      "#  51460.00ns DEBUG    cocotb.top_wrapper                 Observed Weight Channel request out features: 10000000000, in features 10000000000  \n",
      "#  52520.00ns DEBUG    cocotb.top_wrapper                 Transaction {'start_address': 10496, 'data': [], 'expected_length': 2}\n",
      "#  52545.00ns ERROR    ..nitor.monitor_write_transactions Exception raised by this forked coroutine\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/librt.so.1 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/libdl.so.2 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/libm.so.6 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/libpthread.so.0 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/libc.so.6 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/ld-linux-x86-64.so.2 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/libnss_sss.so.2 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libcocotbvpi_modelsim.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libgpi.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libgpilog.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/../../../../libstdc++.so.6 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/../../../../libgcc_s.so.1 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libcocotbutils.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libembed.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/libpython3.11.so.1.0 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /lib64/libutil.so.1 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libcocotb.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/libs/libpygpilog.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_typing.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/math.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_bisect.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_random.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_sha512.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libffi.so.8 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_struct.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/cocotb/simulator.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_opcode.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_decimal.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_hashlib.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libcrypto.so.3 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_blake2.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_heapq.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_socket.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/select.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/array.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/fcntl.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_posixsubprocess.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_ssl.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libssl.so.3 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/binascii.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libz.so.1 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_contextvars.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_elementtree.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/pyexpat.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libexpat.so.1 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/zlib.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_bz2.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libbz2.so.1.0 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_lzma.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../liblzma.so.5 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_uuid.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/../../libuuid.so.1 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/unicodedata.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_csv.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_datetime.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# vsim_stacktrace.vstf written\n",
      "# Current time Fri Aug 30 14:26:46 2024\n",
      "# ModelSim SE Stack Trace\n",
      "# Program = vsim\n",
      "# Id = \"2019.2\"\n",
      "# Version = \"2019.04\"\n",
      "# Date = \"Apr 16 2019\"\n",
      "# Platform = \"linux_x86_64\"\n",
      "# Signature = a9dece1699e369c798442ff51861b880\n",
      "# 0    0x000000000080d263: '<unknown (@0x80d263)>'\n",
      "# 1    0x000000000080d5db: '<unknown (@0x80d5db)>'\n",
      "# 2    0x0000000000f315cd: '<unknown (@0xf315cd)>'\n",
      "# 3    0x000000000050d685: '<unknown (@0x50d685)>'\n",
      "# 4    0x000000000068cf33: '<unknown (@0x68cf33)>'\n",
      "# 5    0x0000000000b90c03: '<unknown (@0xb90c03)>'\n",
      "# 6    0x0000000000b954bb: '<unknown (@0xb954bb)>'\n",
      "# 7    0x0000000000b971ee: '<unknown (@0xb971ee)>'\n",
      "# 8    0x0000000000e4612d: '<unknown (@0xe4612d)>'\n",
      "# 9    0x000000000156939d: '<unknown (@0x156939d)>'\n",
      "# 10   0x000000000156d7f6: '<unknown (@0x156d7f6)>'\n",
      "# 11   0x000000000156eee1: '<unknown (@0x156eee1)>'\n",
      "# 12   0x000000000156f246: '<unknown (@0x156f246)>'\n",
      "# 13   0x0000000001072936: '<unknown (@0x1072936)>'\n",
      "# 14   0x000000000160f14f: '<unknown (@0x160f14f)>'\n",
      "# 15   0x0000000001662cb7: '<unknown (@0x1662cb7)>'\n",
      "# 16   0x0000000001625bb7: '<unknown (@0x1625bb7)>'\n",
      "# 17   0x0000000001625e99: '<unknown (@0x1625e99)>'\n",
      "# 18   0x0000000001439ffd: '<unknown (@0x1439ffd)>'\n",
      "# 19   0x0000000000b68a1c: '<unknown (@0xb68a1c)>'\n",
      "# End of Stack Trace\n",
      "\n",
      "\n",
      "** Fatal: (SIGSEGV) Bad pointer access. Closing vsimk.\n",
      "** Fatal: vsimk is exiting with code 211.\n",
      "Exit codes are defined in the \"Error and Warning Messages\"\n",
      "appendix of the ModelSim User's Manual.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_json.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/numpy/_core/../../../../libcblas.so.3 : text section appears to be mapped multiple times.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/numpy/_core/../../../.././libgfortran.so.5 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/numpy/_core/../../../../././libquadmath.so.0 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_pickle.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/numpy/_core/_multiarray_tests.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/numpy/linalg/_umath_linalg.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/matplotlib/_c_internal_utils.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/PIL/_imaging.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/PIL/../pillow.libs/libtiff-4da6744b.so.6.0.2 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/PIL/../pillow.libs/libjpeg-f391b078.so.62.4.0 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/PIL/../pillow.libs/libopenjp2-05423b53.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/PIL/../pillow.libs/libxcb-80c5a837.so.1.1.0 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/PIL/../pillow.libs/liblzma-13fa198c.so.5.4.5 : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/matplotlib/_path.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/matplotlib/ft2font.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/kiwisolver/_cext.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/matplotlib/_image.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libtorch_global_deps.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/_C.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libtorch_python.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libshm.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libtorch.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libc10_cuda.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/site-packages/torch/lib/libc10.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/grp.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/cmath.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_multiprocessing.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_queue.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "# ** Warning: (vsim-3116) Problem reading symbols from /home/aw1223/anaconda3/envs/ample/lib/python3.11/lib-dynload/_statistics.cpython-311-x86_64-linux-gnu.so : module was loaded at an absolute address.\n",
      "#  52545.00ns ERROR    cocotb.scheduler                   Failing test at simulator request before test run completion: Simulator shut down prematurely\n",
      "# End time: 14:26:47 on Aug 30,2024, Elapsed time: 0:00:17\n",
      "# Errors: 0, Warnings: 90\n",
      "make[1]: Leaving directory '/home/aw1223/ip/agile/hw/sim'\n",
      "| Component   | Metric       |   Value |\n",
      "|:------------|:-------------|--------:|\n",
      "| fpga        | Fpga Latency | 0.23237 |\n",
      "tensor([[-0.2077,  0.5414, -0.9323, -0.0156, -0.9242, -1.4551,  1.1447,  0.4894,\n",
      "         -1.2190, -2.1434,  0.8033, -1.3588, -1.2911,  1.1237,  0.1125,  0.3963,\n",
      "          0.5141,  0.9954,  0.0770,  1.1795, -1.2423, -0.2647, -0.4984, -1.0298,\n",
      "         -2.2073,  1.7572, -0.5169,  1.4884,  1.1717, -1.6047, -1.0268,  1.6982],\n",
      "        [-1.8529,  1.6048, -0.6758,  0.4821, -0.7355,  2.7671,  0.6574,  0.1128,\n",
      "         -0.8243,  0.9435, -0.5425,  1.3881, -0.3223,  2.2466,  0.2966, -0.3389,\n",
      "         -0.3565,  1.7096,  0.6556,  0.9367, -0.2056, -0.2510, -0.0603, -0.9671,\n",
      "          2.2658,  0.2223, -2.8040, -0.8261,  0.5246, -2.1695, -0.0143,  0.8446],\n",
      "        [ 6.7803,  5.0186,  5.3118,  6.6033,  5.3946,  4.0241,  5.3939,  5.0030,\n",
      "          4.8918,  3.9511,  6.0363,  4.9824,  5.2248,  6.5227,  5.0742,  4.1273,\n",
      "          4.4227,  5.2284,  5.5740,  6.3545,  4.3107,  5.2994,  5.3288,  5.3148,\n",
      "          5.8875,  5.0108,  5.8182,  4.4662,  5.2387,  6.0119,  4.5336,  5.5987]])\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 1\n",
      "[tensor([[-0.0232, -0.1628,  0.1180,  0.9688, -1.3416, -0.4082,  0.4390, -0.5858,\n",
      "          0.8369,  0.8001,  0.1626,  0.0341, -0.5942, -0.1142,  0.3318,  0.7806,\n",
      "         -0.6352, -0.1492,  0.0673, -0.7630, -0.0119,  0.5197, -0.1820, -0.1857,\n",
      "         -0.0556, -0.8972, -0.0540, -0.4161,  0.7000,  0.8395,  0.3146,  0.1167],\n",
      "        [-0.4574, -0.2689,  0.2395, -0.1866,  0.5837, -0.1758,  0.1316, -0.6019,\n",
      "          0.6827,  0.9399,  0.6414,  0.4476,  0.3151,  0.1674, -0.2589, -0.1336,\n",
      "         -0.4787, -1.5682, -0.7751,  0.7476,  0.4547,  1.1180, -0.3984, -0.3965,\n",
      "          0.3853,  0.3864,  0.4101, -0.4491,  0.7818,  0.5696,  0.1923,  0.3064],\n",
      "        [-2.1716, -2.4009,  1.6866, -1.3843, -2.3930,  0.7621,  1.8115,  5.5836,\n",
      "         -1.1742, -0.8450, -1.9967,  0.4845, -3.2447, -2.1810,  1.1584,  0.1502,\n",
      "         -0.6513,  0.0680,  0.4742,  2.8633, -3.8617, -5.6893, -0.0496,  2.9577,\n",
      "         -2.6307,  1.0316, -1.4899,  0.9373, -4.1918, -0.8763,  0.0690, -3.8519]],\n",
      "       grad_fn=<MmBackward0>)]\n",
      "weights trace\n",
      "MLP_Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      ")\n",
      "weights tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]])\n",
      "----weights---;\n",
      "outputs_sub_model lin 2\n",
      "[tensor([[-3.8640e-01,  7.0930e-06,  4.4277e-01, -3.7034e-01,  6.8428e-02,\n",
      "         -4.1635e-01,  4.8004e-01,  7.0448e-02,  1.7546e-01,  4.8320e-01,\n",
      "         -2.8791e-01, -1.4448e-01, -2.5966e-01,  1.0892e-01,  2.4666e-01,\n",
      "         -2.8190e-01,  3.7408e-01, -3.5782e-02, -2.2300e-01, -1.6119e-01,\n",
      "         -2.2833e-01,  6.2510e-01,  5.7025e-01,  3.1323e-01, -1.2398e-01,\n",
      "          2.2021e-01,  1.5653e-01,  2.9260e-01, -1.9918e-01, -3.0719e-01,\n",
      "         -4.3428e-01,  1.1919e-01],\n",
      "        [-3.2743e-01,  3.3594e-01,  1.9756e-01, -3.5456e-01, -3.4186e-01,\n",
      "         -2.8791e-01, -4.7120e-01,  3.6984e-01,  2.3701e-01,  4.5589e-01,\n",
      "         -4.8585e-01, -8.1758e-02,  3.9632e-01, -2.6830e-01,  5.4913e-01,\n",
      "          2.6451e-01, -1.7801e-02,  5.0757e-01,  5.7962e-03,  4.7940e-01,\n",
      "          3.9499e-01, -2.1103e-01, -3.8050e-01,  2.4722e-01, -1.1495e-01,\n",
      "          5.5509e-01, -1.0508e-01, -3.6900e-01, -2.6037e-01, -2.0445e-01,\n",
      "          4.9247e-01,  1.5074e-01],\n",
      "        [ 2.5450e-01, -1.5792e+00, -5.5176e-01, -1.7937e-01,  1.0951e+00,\n",
      "         -7.2113e-01, -8.2971e-01, -1.0739e+00, -1.0610e+00, -1.3977e+00,\n",
      "          3.3180e-01, -1.0131e-01, -1.6183e+00,  7.1987e-01, -1.4779e+00,\n",
      "         -8.4460e-01, -7.0296e-01, -1.2418e-02,  2.1482e+00, -2.1476e+00,\n",
      "          8.4754e-01,  4.0650e-01, -1.2422e+00, -7.7162e-02,  1.4960e-01,\n",
      "         -2.3849e-01,  4.2979e-02,  4.0451e-01,  2.7452e+00,  1.4226e+00,\n",
      "         -6.7968e-01, -1.9930e-01]], grad_fn=<MmBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "make[1]: *** [Makefile.modelsim:224: run_sim] Error 211\n",
      "make: *** [Makefile:11: run_sim] Error 2\n"
     ]
    }
   ],
   "source": [
    "out = model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph from /home/aw1223/ip/agile/hw/sim/layer_config/graph.pth\n",
      "loading model from /home/aw1223/ip/agile/hw/sim/layer_config/model.pt\n",
      "model\n",
      "layers.0.layers.0.weight tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]]) name,param\n",
      "model\n",
      "layers.1.layers.0.weight tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]]) name,param\n",
      "model\n",
      "linear1.layers.0.weight tensor([[-0.0084,  0.0304,  0.1117,  ..., -0.0922, -0.1008, -0.1613],\n",
      "        [ 0.0309,  0.0559,  0.0508,  ...,  0.0325,  0.0554, -0.1621],\n",
      "        [ 0.0070,  0.0611,  0.0005,  ..., -0.1199, -0.1162, -0.1191],\n",
      "        ...,\n",
      "        [-0.0395,  0.0118, -0.0175,  ..., -0.0227,  0.0451, -0.0544],\n",
      "        [-0.0576,  0.0287, -0.0501,  ..., -0.0898,  0.1367,  0.1310],\n",
      "        [-0.0451, -0.0412, -0.0661,  ..., -0.0585,  0.0148, -0.1411]]) name,param\n",
      "model\n",
      "linear2.layers.0.weight tensor([[ 0.0710,  0.0266,  0.0160,  ..., -0.0566, -0.1146, -0.0305],\n",
      "        [ 0.1374, -0.1612, -0.1360,  ..., -0.1006, -0.0068,  0.0423],\n",
      "        [-0.1172, -0.0600, -0.0220,  ...,  0.1420,  0.1664,  0.1044],\n",
      "        ...,\n",
      "        [-0.0066,  0.0116, -0.1186,  ..., -0.1546, -0.1020, -0.1366],\n",
      "        [ 0.1754,  0.1621,  0.0796,  ..., -0.0564,  0.0948, -0.0879],\n",
      "        [ 0.1100,  0.1128, -0.0487,  ...,  0.1129,  0.0227, -0.0095]]) name,param\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_expected_outputs(model,data):\n",
    "  ####Remove bias from the model, TODO Add biases####\n",
    "  state_dict = model.state_dict()\n",
    "  for name, param in state_dict.items():\n",
    "      print('model')\n",
    "      print(name,param,'name,param')\n",
    "      if 'bias' in name:\n",
    "          # Reset the bias tensor to all zeros\n",
    "          state_dict[name] = torch.zeros_like(param)\n",
    "\n",
    "  model.load_state_dict(state_dict)\n",
    "  \n",
    "\n",
    "  model_input = data\n",
    "  # if edge_attr is not None:\n",
    "  #     model_input = model_input + (edge_attr,)  \n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      # if edge_attr is not None:\n",
    "      outputs,x = model(*model_input)\n",
    "      # else:\n",
    "      #     output = model(x, edge_index)\n",
    "      # output = model(x, edge_index, edge_attr) if edge_attr is not None else model(x, edge_index)\n",
    "      # a = b\n",
    "  del model\n",
    "\n",
    "  return outputs\n",
    "\n",
    "def load_jit_model():\n",
    "  config_path =  os.environ.get(\"WORKAREA\") + \"/hw/sim/layer_config/\"\n",
    "\n",
    "  model_path = config_path + 'model.pt'\n",
    "  print('loading model from',model_path)\n",
    "\n",
    "  model = torch.jit.load(model_path)\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "def load_graph():\n",
    "  config_path =  os.environ.get(\"WORKAREA\") + \"/hw/sim/layer_config/\"\n",
    "\n",
    "  graph_path = config_path + 'graph.pth'\n",
    "  print('loading graph from',graph_path)\n",
    "\n",
    "  graph = torch.load(graph_path)\n",
    "  input_data = graph['input_data']\n",
    "  # x_loaded = input_data['x']\n",
    "  # edge_index_loaded = input_data['edge_index']\n",
    "\n",
    "  # # Check if edge attributes are present and load them if they are\n",
    "  # edge_attr_loaded = input_data.get('edge_attr', None)\n",
    "  \n",
    "  return input_data #(x_loaded, edge_index_loaded, edge_attr_loaded)\n",
    "  \n",
    "\n",
    "inputs = load_graph()\n",
    "model = load_jit_model()\n",
    "expected_outputs = get_expected_outputs(model,inputs)\n",
    "# expected_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
