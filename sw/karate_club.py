# -*- coding: utf-8 -*-
"""karate_club.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1stVpi7Vpe5NuZdQFYp3I0_d3lmx0hTKl

### Imports
"""

import os
import random
import struct

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

import matplotlib.pyplot as plt
import networkx as nx

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

os.environ['TORCH'] = torch.__version__

# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html --quiet
# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html --quiet
# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
# !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.10.0+${CUDA}.html
# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git --quiet

from torch_geometric.datasets import KarateClub
from torch_geometric.utils import to_networkx
from torch_geometric.nn import Node2Vec, GCNConv
import torch_cluster
random_walk = torch.ops.torch_cluster.random_walk

"""### Utilities"""

def visualize_graph(graph):
  nx_graph = to_networkx(graph)
  pos = nx.spring_layout(nx_graph)
  nx.draw(nx_graph, pos, with_labels=True)
  plt.show()

def int_list_to_byte_list(in_list, pad=False, end_offset=None):
  '''
    Convert to list of bytes in hex
  '''
  memory_hex = np.array([f"{dest_node:08x}" for dest_node in in_list])
  memory_hex = [s[i:i+2] for s in memory_hex for i in range(0, 8, 2)]
  if (pad and end_offset is not None):
    memory_hex += ['00'] * (end_offset - len(memory_hex))
  return memory_hex

def float_list_to_byte_list(in_list, pad=False, end_offset=None):
  hex_list = [struct.pack('!f', i).hex() for i in in_list]
  hex_list = [s[i:i+2] for s in hex_list for i in range(0, 8, 2)]
  if (pad and end_offset is not None):
    hex_list += ['00'] * (end_offset - len(hex_list))
  return hex_list

def print_memory(memory):
  for i in range(len(memory)//64):
    # print(f"idx {i*64} to {(i+1)*64}")
    print(''.join(memory[i*64:(i+1)*64]))
  # print(f"idx {64*(len(memory)//64)} to {len(memory)}")
  print(''.join(memory[64*(len(memory)//64):]))

"""### Load graph and generate feature embeddings"""

# Load the dataset
karate = KarateClub()

visualize_graph(karate[0])

n2v_model = Node2Vec(
    karate[0].edge_index,
    embedding_dim=64,
    walk_length=10,
    context_size=5,
    walks_per_node=10,
    num_negative_samples=1,
    sparse=True
).train()

loader = n2v_model.loader(batch_size=128, shuffle=True, num_workers=4)
optimizer = torch.optim.SparseAdam(n2v_model.parameters(), lr=0.01)

# Training loop
for epoch in range(1, 101):
  total_loss = 0
  for pos_rw, neg_rw in loader:
    optimizer.zero_grad()
    loss = n2v_model.loss(pos_rw.to(device), neg_rw.to(device))
    loss.backward()
    optimizer.step()
    total_loss += loss.item()
  if (epoch % 10 == 0):
    print(f"Epoch {epoch}, Loss: {total_loss / len(loader)}")

node_embeddings = n2v_model.embedding.weight
print(float_list_to_byte_list(node_embeddings[0]))

"""### Train GCN network for club classification"""

print(karate[0])

class GCN(nn.Module):
  def __init__(self, in_channels, out_channels):
    super(GCN, self).__init__()
    self.conv1 = GCNConv(in_channels, 16)
    self.conv2 = GCNConv(16, out_channels)

  def forward(self, x, edge_index):
    x = self.conv1(x, edge_index)
    x = F.relu(x)
    x = F.dropout(x, training=self.training)
    x = self.conv2(x, edge_index)
    return F.log_softmax(x, dim=1)

def evaluate(model, x, edge_index, y, mask):
  model.eval()
  with torch.no_grad():
      logits = model(x, edge_index)
      pred = logits.argmax(dim=1)
      correct = pred[mask].eq(y[mask]).sum().item()
      acc = correct / mask.sum().item()
      return acc

model = GCN(in_channels=64, out_channels=4)
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

x = node_embeddings.to(device)
y = karate[0].y.to(device)

# print(karate[0].y)

'''
TO DO: something wrong with training

Use classes from labels = np.asarray([G.nodes[i]['club'] != 'Mr. Hi' for i in G.nodes]).astype(np.int64)

Or: four classes obtained via modularity-based
    clustering, following the `"Semi-supervised Classification with Graph
    Convolutional Networks" <https://arxiv.org/abs/1609.02907>`_ paper.
'''

for epoch in range(1, 201):
  model.train()
  optimizer.zero_grad()
  out = model(x, karate[0].edge_index)
  loss = F.nll_loss(out[karate[0].train_mask], y[karate[0].train_mask])
  loss.backward()
  optimizer.step()

  pred = out.argmax(dim=1)
  correct = pred.eq(y).sum().item()
  train_acc = correct / y[karate[0].train_mask].shape[0]
  # test_acc = evaluate(model, x, karate[0].edge_index, y, karate[0].test_mask)
  if epoch % 10 == 0:
    print(f"Epoch {epoch}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {0}")

"""Get weights"""

weights = model.state_dict()
# print(weights.keys())
conv1_weights = weights["conv1.lin.weight"]
print(conv1_weights)

"""### Defines"""

BYTES_PER_FLOAT = 4

IN_FEATURES = 64
OUT_FEATURES = 64

messages_offset = 640
weights_offset = messages_offset + 34*64*BYTES_PER_FLOAT

"""### Generate Adjacency List"""

# Node IDs and memory offsets of destination node IDs for each source node
node_ids, node_offsets = np.unique(karate[0].edge_index[0], return_index=True)

# List of destination node IDs
adj_list = karate[0].edge_index[1]
print(adj_list.shape)
print(adj_list)

memory_hex = int_list_to_byte_list(adj_list, pad=True, end_offset=messages_offset)
print_memory(memory_hex)

"""### Incoming messages"""

for node in range(34):
  memory_hex += float_list_to_byte_list(node_embeddings[node])

"""### Random weights"""

weights = np.zeros((OUT_FEATURES, IN_FEATURES))
for outf in range(OUT_FEATURES):
  weights[outf] = [random.uniform(-2, 2) for _ in range(64)]
  memory_hex += float_list_to_byte_list(weights[outf])
print(weights)

print_memory(memory_hex)